{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd0acdc",
   "metadata": {},
   "source": [
    "# Linear Regression using Tensorflow\n",
    "\n",
    "In this notebook, we make a custom loss funtion for tensor flow, then do a linear regression on our features for various optimizers (mostly Adam, after preliminary testing) in tensor flow models.\n",
    "\n",
    "Result: The best linear regression gives an mae of 1.210 on k-fold validation and uses the following features: \n",
    "-az_t_pred\n",
    "-ze_t_pred\n",
    "-low_cluster (cutoff of 2)\n",
    "-high_cluster (cutoff of 9)\n",
    "-mse_cat (cutoff of 721)\n",
    "-x_skew (cutoff of .9)\n",
    "-y_skew\n",
    "-z_skew\n",
    "It also uses the Adam optimizer and 9 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3954b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\k_vsl\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f7bc7",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9b1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def get_mae(az_true, zen_true, az_pred, zen_pred): \n",
    "    \"\"\"\n",
    "    Given a predicted and true azimuth and zenith, compute the mae (mean angular error)\n",
    "    \"\"\"    \n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = tf.math.sin(az_true)\n",
    "    ca1 = tf.math.cos(az_true)\n",
    "    sz1 = tf.math.sin(zen_true)\n",
    "    cz1 = tf.math.cos(zen_true)\n",
    "    \n",
    "    sa2 = tf.math.sin(az_pred)\n",
    "    ca2 = tf.math.cos(az_pred)\n",
    "    sz2 = tf.math.sin(zen_pred)\n",
    "    cz2 = tf.math.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
    "    # that might otherwise occure from the finite precision of the sine and cosine functions\n",
    "    scalar_prod = tf.clip_by_value(scalar_prod, -1.0, 1.0)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return tf.reduce_mean(tf.abs(tf.acos(scalar_prod)))\n",
    "\n",
    "def mae(y_true, y_pred): \n",
    "    #return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    #print(type(y_true))\n",
    "    ta = tf.gather(y_true, 0)\n",
    "    tz = tf.gather(y_true, 1)\n",
    "    pa = tf.gather(y_pred, 0)\n",
    "    pz = tf.gather(y_pred, 1)\n",
    "    return get_mae(ta, tz, pa, pz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fe616",
   "metadata": {},
   "source": [
    "# Set up train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432123a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "event_data = pd.read_csv(\"C:/Users/k_vsl/Documents/Erdos/Boot Camp/ice-cube-katja/features-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c96cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V['x_skew'] = [(val > .9) for val in V.per_x]\n",
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V['y_skew'] = [(val > .9) for val in V.per_y]\n",
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V['z_skew'] = [(val > .9) for val in V.per_z]\n",
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V['low_cluster'] = [(c < 2) for c in V.num_clusters]\n",
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V['high_cluster'] = [(c > 9) for c in V.num_clusters]\n",
      "C:\\Users\\k_vsl\\AppData\\Local\\Temp\\ipykernel_10980\\586390680.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  V.replace({False: 0, True: 1}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# X contains all categorical variables with cutoff of .5 for skew\n",
    "# V contains a subset of categorical variables, where skew has a cutoff of .9 and we categorize into low, medium, and high clusters\n",
    "X = event_data\n",
    "X = X.set_index(\"event_id\")\n",
    "y = event_data[['event_id', 'az_true', 'ze_true']]\n",
    "y = y.set_index(\"event_id\")\n",
    "X = X[['az_t_pred', 'ze_t_pred', 'cat_x', 'cat_y', 'cat_z', 'mse_cat', 'cat_1.0',\n",
    "       'cat_2.0', 'cat_3.0', 'cat_4.0', 'cat_5.0', 'cat_6.0', 'cat_7.0',\n",
    "       'cat_8.0', 'cat_9.0', 'cat_10.0']]\n",
    "V = event_data[['az_t_pred', 'ze_t_pred', 'num_clusters','mse_cat','per_x', 'per_y', 'per_z']]\n",
    "V['x_skew'] = [(val > .9) for val in V.per_x]\n",
    "V['y_skew'] = [(val > .9) for val in V.per_y]\n",
    "V['z_skew'] = [(val > .9) for val in V.per_z]\n",
    "V['low_cluster'] = [(c < 2) for c in V.num_clusters]\n",
    "V['high_cluster'] = [(c > 9) for c in V.num_clusters]\n",
    "V.replace({False: 0, True: 1}, inplace=True)\n",
    "w = y\n",
    "V = V[['az_t_pred', 'ze_t_pred', 'mse_cat', 'x_skew', 'y_skew', 'z_skew', 'low_cluster', 'high_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08009ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train and test\n",
    "\n",
    "# Separate out a final training set\n",
    "# random seed = 134\n",
    "# test size = 25%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                             shuffle = True,\n",
    "                                                             random_state = 134, \n",
    "                                                             test_size = .25)\n",
    "V_train, V_test, w_train, w_test = train_test_split(V, w, \n",
    "                                                             shuffle = True,\n",
    "                                                             random_state = 134, \n",
    "                                                             test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8cd3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "# this cell imitates the erdos lectures notes on kfold cross validation , k = 5\n",
    "# random seed to all splits random_seed = 134\n",
    "kfold = KFold(n_splits = 5,\n",
    "             shuffle = True,\n",
    "             random_state = 134)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c7c2b",
   "metadata": {},
   "source": [
    "# Helper training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc91c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which runs through optimizers, various epochs, kfold validation\n",
    "def tensor_train(X,  y, optimizers, shape, epochs): \n",
    "    n = len(optimizers)\n",
    "    losses = np.zeros((n,10,5))\n",
    "    i = 0\n",
    "    for opt in optimizers: \n",
    "        print(\"Trying Optimizer \" + str(opt))\n",
    "        k = 0\n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "                    \n",
    "            ## get the kfold training data\n",
    "            X_train = X.iloc[train_index,:]\n",
    "            y_train = y.iloc[train_index]\n",
    "\n",
    "            ## get the holdout data\n",
    "            X_holdout = X.iloc[test_index,:]\n",
    "            y_holdout = y.iloc[test_index]\n",
    "                    \n",
    "            j = 0\n",
    "            for e in epochs: \n",
    "\n",
    "                ## Fit the data\n",
    "                model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(shape,))])\n",
    "                model.compile(optimizer = opt, loss = mae)\n",
    "                model.fit(X_train, y_train, epochs=e)\n",
    "                loss = model.evaluate(X_holdout, y_holdout)\n",
    "\n",
    "                losses[i][j][k] = loss\n",
    "                j += 1\n",
    "                    \n",
    "            k += 1\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "    return losses\n",
    "\n",
    "# Function which tests one optimizer with a given number of epochs\n",
    "def tensor_train_spec(X,y, opt, shape, epoch):\n",
    "    losses = np.zeros(5)\n",
    "    i = 0\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        ## get the kfold training data\n",
    "        X_train = X.iloc[train_index,:]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        ## get the holdout data\n",
    "        X_holdout = X.iloc[test_index,:]\n",
    "        y_holdout = y.iloc[test_index]\n",
    "        model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(shape,))])\n",
    "        model.compile(optimizer = opt, loss = mae)\n",
    "        model.fit(X_train, y_train, epochs=epoch)\n",
    "        loss = model.evaluate(X_holdout, y_holdout)\n",
    "        losses[i] = loss\n",
    "        i +=1\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33acaf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a87034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Optimizer Adam\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2365\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2094\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 17s 4ms/step - loss: 1.2129\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.1990\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.1998\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 1.2057\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 14s 3ms/step - loss: 1.3949\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 12s 3ms/step - loss: 1.2326\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.2264\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2159\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2241\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2260\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2172\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2226\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.2062\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2157\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 1.2154\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 19s 4ms/step - loss: 1.4224\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2489\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2196\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2201\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2155\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2230\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2174\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.2136\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2258\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2122\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2158\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2160\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2256\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2194\n",
      "Epoch 15/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2099\n",
      "938/938 [==============================] - 4s 3ms/step - loss: 1.2192\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2463\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2261\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2100\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2094\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2142\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 1.1918\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 18s 4ms/step - loss: 1.2885\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2185\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2092\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2079\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2064\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2044\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2037\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.1989\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.1973\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2047\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.1910\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.3009\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2383\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2182\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2294\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2170\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2193\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2114\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2264\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 1.2200\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2145\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2173\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2122\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2234\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2230\n",
      "Epoch 15/15\n",
      "3750/3750 [==============================] - 22s 6ms/step - loss: 1.2099\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 1.2024\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2942\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2434\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.2190\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2282\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 22s 6ms/step - loss: 1.2181\n",
      "938/938 [==============================] - 5s 4ms/step - loss: 1.2322\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 23s 5ms/step - loss: 1.2789\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 32s 9ms/step - loss: 1.2192\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2122\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2001\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2059\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2133\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2074\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2055\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.1967\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.2106\n",
      "938/938 [==============================] - 9s 9ms/step - loss: 1.2111\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 38s 9ms/step - loss: 1.2603\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.2143\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2097\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2009\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2016\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2066\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.2104\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.1894\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 36s 10ms/step - loss: 1.2083\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 22s 6ms/step - loss: 1.1983\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2062\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2065\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2073\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 1.2045\n",
      "Epoch 15/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 4s 4ms/step - loss: 1.2094\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.2688\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 1.2313\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 33s 9ms/step - loss: 1.2109\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2054\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2104\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 1.2066\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 17s 4ms/step - loss: 1.3152\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.2168\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 25s 7ms/step - loss: 1.2067\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2037\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2008\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.1997\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 1.2068\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 55s 15ms/step - loss: 1.2009\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2143\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.1933\n",
      "938/938 [==============================] - 7s 6ms/step - loss: 1.2034\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 28s 7ms/step - loss: 1.3312\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 22s 6ms/step - loss: 1.2574\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2371\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 35s 9ms/step - loss: 1.2333\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 30s 8ms/step - loss: 1.2214\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 29s 8ms/step - loss: 1.2175\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 17s 4ms/step - loss: 1.2102\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2199\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 32s 8ms/step - loss: 1.2202\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 34s 9ms/step - loss: 1.2159\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 35s 9ms/step - loss: 1.2275\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2197\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 28s 8ms/step - loss: 1.2138\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 25s 7ms/step - loss: 1.2164\n",
      "Epoch 15/15\n",
      "3750/3750 [==============================] - 33s 9ms/step - loss: 1.2116\n",
      "938/938 [==============================] - 9s 8ms/step - loss: 1.2197\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 37s 8ms/step - loss: 1.3092\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 29s 8ms/step - loss: 1.2252\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 1.2165\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2110\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.2094\n",
      "938/938 [==============================] - 10s 9ms/step - loss: 1.2256\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 23s 5ms/step - loss: 1.3296\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.2289\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 48s 13ms/step - loss: 1.2271\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 57s 15ms/step - loss: 1.2292\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2314\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2101\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.2095\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 35s 9ms/step - loss: 1.2128\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 51s 14ms/step - loss: 1.2166\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 1.2220\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 1.2395\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.3414\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.2337\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.2036\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 62s 17ms/step - loss: 1.2074\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 45s 12ms/step - loss: 1.1970\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.2003\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 35s 9ms/step - loss: 1.2096\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.1960\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.1999\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.2092\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 67s 18ms/step - loss: 1.1924\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.2078\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.2130\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 9s 2ms/step - loss: 1.2065\n",
      "Epoch 15/15\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 1.2084\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.2290\n",
      "Trying Optimizer Adadelta\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.4588\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 54s 14ms/step - loss: 1.4554\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 30s 8ms/step - loss: 1.4638\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 1.4634\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 17s 5ms/step - loss: 1.4595\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 1.4685\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 21s 5ms/step - loss: 1.4788\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 19s 5ms/step - loss: 1.5105\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.4985\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.4991\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.4829\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 59s 16ms/step - loss: 1.4959\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 23s 6ms/step - loss: 1.4891\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.4793\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 16s 4ms/step - loss: 1.4977\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.4759\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 1.4826\n",
      "Epoch 1/15\n",
      "3750/3750 [==============================] - 24s 6ms/step - loss: 1.4812\n",
      "Epoch 2/15\n",
      "3750/3750 [==============================] - 64s 17ms/step - loss: 1.4882\n",
      "Epoch 3/15\n",
      "3750/3750 [==============================] - 25s 7ms/step - loss: 1.4669\n",
      "Epoch 4/15\n",
      "3750/3750 [==============================] - 14s 4ms/step - loss: 1.4820\n",
      "Epoch 5/15\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.4792\n",
      "Epoch 6/15\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.4739\n",
      "Epoch 7/15\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.4865\n",
      "Epoch 8/15\n",
      "3750/3750 [==============================] - 15s 4ms/step - loss: 1.4655\n",
      "Epoch 9/15\n",
      "3750/3750 [==============================] - 27s 7ms/step - loss: 1.4845\n",
      "Epoch 10/15\n",
      "3750/3750 [==============================] - 68s 18ms/step - loss: 1.4822\n",
      "Epoch 11/15\n",
      "3750/3750 [==============================] - 21s 6ms/step - loss: 1.4739\n",
      "Epoch 12/15\n",
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.4703\n",
      "Epoch 13/15\n",
      "3750/3750 [==============================] - 13s 4ms/step - loss: 1.4871\n",
      "Epoch 14/15\n",
      "3750/3750 [==============================] - 12s 3ms/step - loss: 1.4821\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 13s 3ms/step - loss: 1.4795\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4584\n",
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 17s 4ms/step - loss: 1.5007\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 1.4839\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 55s 15ms/step - loss: 1.4895\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 20s 5ms/step - loss: 1.4806\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 18s 5ms/step - loss: 1.4819\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 1.4602\n",
      "Epoch 1/10\n",
      " 667/3750 [====>.........................] - ETA: 12s - loss: 1.4226"
     ]
    }
   ],
   "source": [
    "# Look at X\n",
    "epochs = [5,10,15]\n",
    "optimizers = ['Adam', 'Adadelta','sgd']\n",
    "losses = tensor_train(X_train, y_train, optimizers, 16, epochs)\n",
    "n = len(optimizers)\n",
    "m = len(epochs)\n",
    "means = np.zeros((n,m))\n",
    "for i in range(0,n): \n",
    "    for j in range(0,m): \n",
    "        means[i][j] = losses[i][j].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e561467",
   "metadata": {},
   "source": [
    "Conclusion: Adam is the best optimizer to use for our scenario. We may want to cut down on the number of categorical variables to prevent overfitting...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(optimizers)\n",
    "m = len(epochs)\n",
    "means = np.zeros((n,m))\n",
    "for i in range(0,n): \n",
    "    for j in range(0,m): \n",
    "        means[i][j] = losses[i][j].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at V\n",
    "epochs = [6,9, 13,26]\n",
    "optimizers = ['Adam']\n",
    "mae = np.zeros(len(epochs))\n",
    "i = 0\n",
    "for e in epochs: \n",
    "    loss = tensor_train_spec(V_train,w_train, 'Adam', 8, e)\n",
    "    print(loss)\n",
    "    print(loss.mean())\n",
    "    mae[i] = loss.mean()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c701c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result: 9 epochs appears to be the best\n",
    "    \n",
    "6 epochs: 1.2728947401046753\n",
    "9 epochs: 1.2104717016220092\n",
    "13 epochs: 1.2120121955871581\n",
    "26 epochs: 1.265834665298462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d77f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate learning rate\n",
    "# Didn't end up doing this fully as preliminary results were not positive\n",
    "decay_steps = 1000\n",
    "initial_learning_rate = .0001\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps, warmup_target= None,\n",
    "    warmup_steps=0\n",
    ")\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "loss = tensor_train_spec(V_train,w_train, opt, 8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d06c2",
   "metadata": {},
   "source": [
    "# Test final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d039432",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835eb21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
