{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad061364-205b-44ca-a026-d2e204725f33",
   "metadata": {},
   "source": [
    "The neural net is trained on a combination of provided and original features. The file sensor_geometry.csv contains the (x,y,z) positions of the sensors in the IceCube detector. The file train_meta.parquet provides additional information about each neutrino event. Most importantly, it contains the true azimuth and zenith of the incoming neutrino, which we train against. The neutrino events are stored in files of the form batch_##.parquet, where ## ranges from 1 to 660. Each batch file contains the information on ~200,000 neutrino events. A single neutrino event consists of an arbitrary number of rows in the parquet file, with each row corresponding to a single sensor activation during the neutrino event. \n",
    "\n",
    "The all-features.csv file consist of original features derived from the linear regression and clustering computations. We utilize just an initial guess of the azimuth and zenith and an estimate of the number of sensor activation clusters in the event. \n",
    "\n",
    "This is a simple fully connected neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63657967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take Care of the Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f6f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set parent directories for:\n",
    "\n",
    "sensor_geometry.csv\n",
    "batch_##.parquet\n",
    "train_meta.parquet\n",
    "all-features.csv\n",
    "\n",
    "respectively. \n",
    "\n",
    "'''\n",
    "\n",
    "pre_dir = '/opt/app/data/erdos-data/'\n",
    "sensor_geom_dir = '/opt/app/data/erdos-data/'\n",
    "meta_dir = '/opt/app/data/erdos-data/'\n",
    "batch_dir = '/opt/app/data/erdos-data/train/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231cfc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set paths using specified directories. \n",
    "pre_path=pre_dir+\"all-features.csv\"\n",
    "sensor_geom_path=sensor_geom_dir+\"sensor_geometry.csv\"\n",
    "meta_path=meta_dir+\"train_meta.parquet\"\n",
    "\n",
    "#Load precompiled features and sensor geometry\n",
    "pre_feature=pd.read_csv(pre_path)\n",
    "sensor_geom = pd.read_csv(sensor_geom_path)\n",
    "\n",
    "\n",
    "#Load metadata, if it is not already loaded. \n",
    "try: meta\n",
    "except NameError: meta=pd.read_parquet(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6ec88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Default Device to CUDA or CPU\n",
    "#to be called in .to(device) to ensure all pytorch Tensors are on the same device\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd97c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Torch Dataset Wrapper.\n",
    "Initialization takes the filename, the sensor geometry file name, the batch ID, and the auxiliary flag.\n",
    "'''\n",
    "class NeutrinoDataset(Dataset):\n",
    "    def __init__(self, batch_filename, sensor_file_name, batch_id, aux):\n",
    "        \n",
    "        #save sensor_geometry and meta_data for the given patch\n",
    "        self.sensor_geom = pd.read_csv(sensor_file_name)\n",
    "        self.vals_df = meta[meta.batch_id==batch_id]\n",
    "        \n",
    "        #Loads the parquet file as a panda dataframe and filter by aux flag\n",
    "        self.dataframe = pd.read_parquet(batch_filename)\n",
    "        self.dataframe = self.dataframe[self.dataframe.auxiliary==aux]\n",
    "        \n",
    "        #set the number of features per sensor. \n",
    "        #Currently, first and last time, cumulative charge, scattering flag, x,y,z.  \n",
    "        self.num_features = 5160*(3+1+3) \n",
    "        \n",
    "        #Number of neutrino events in the data frame\n",
    "        self.num_events = self.dataframe.index.nunique()\n",
    "        \n",
    "        #Since an event can span multiple rows, save the unique indices, to obtain event_ids.  \n",
    "        self.unique_indices = np.unique(self.dataframe.index)\n",
    "        \n",
    "        \n",
    "    #Return number of Neutrino Events in the Dataset (not the number of rows)\n",
    "    def __len__(self):\n",
    "        return self.num_events\n",
    "    \n",
    "    #Get the ith Neutrino Event, based on the ith unique event id\n",
    "    def __getitem__(self, i):\n",
    "        df = self.dataframe\n",
    "        sg = self.sensor_geom\n",
    "        \n",
    "        #Get the event id corresponding to the ith unique index\n",
    "        event_id=self.unique_indices[i]\n",
    "        \n",
    "        #Load the ith neutrino event\n",
    "        event=df.loc[event_id]\n",
    "        \n",
    "        #Load the metavalues associated to the event\n",
    "        meta_vals = np.array(\n",
    "            self.vals_df.loc[self.vals_df['event_id'] == event_id])[0].astype(float)\n",
    "        \n",
    "        #Convert the event into an array of pulses\n",
    "        pulse_array = np.array(event)\n",
    "        \n",
    "        #Creates a 5160x(1+ num_features) array\n",
    "        #First column is sensor_id, subsequent entries will correspond to sensor features\n",
    "        pulse_array_sensors = np.concatenate((np.expand_dims(np.arange(5160), axis=1), np.zeros([5160, 7])), 1)\n",
    "        \n",
    "        #Find pulse with largest charge\n",
    "        loudest=self.loudest_bang(event)\n",
    "        \n",
    "        #For each pulse in the event, extract the per-sensor features.\n",
    "        #In order, these are first and last time, cumulative charge, scattering distance, x,y,z.  \n",
    "        for pulse in pulse_array:\n",
    "            \n",
    "            #Extract sensor_id\n",
    "            sensor_id=pulse[0]\n",
    "            \n",
    "        \n",
    "            #If this is the first pulse for a detector, store it as the first time.\n",
    "            #Else, store it as the second time. After looping through all pulses, this will be the last time. \n",
    "            if(pulse_array_sensors[sensor_id][1] == 0):\n",
    "                pulse_array_sensors[sensor_id][1] = pulse[1] - meta_vals[2] \n",
    "            else:\n",
    "                pulse_array_sensors[sensor_id][2] = pulse[1] - meta_vals[2]\n",
    "            pulse_array_sensors[sensor_id][3] += pulse[2]\n",
    "        \n",
    "            \n",
    "            #Get sensor xyz\n",
    "            sensor_xyz=self.id_to_xyz(sensor_id)\n",
    "            \n",
    "            #Compute the distance between the sensor and the furthest possible cascade from loudest bang.\n",
    "            scatter=np.linalg.norm( np.array([sensor_xyz[0], sensor_xyz[1], sensor_xyz[2]])-loudest[2])-0.23*(loudest[1]-pulse[1])\n",
    "            pulse_array_sensors[sensor_id][4]=scatter\n",
    "            \n",
    "            #Extract and store the positional information.\n",
    "            xyz_from_id=self.id_to_xyz(sensor_id)\n",
    "            pulse_array_sensors[sensor_id][5]=xyz_from_id[0]\n",
    "            pulse_array_sensors[sensor_id][6]=xyz_from_id[1]\n",
    "            pulse_array_sensors[sensor_id][7]=xyz_from_id[2]\n",
    "            \n",
    "        #Extract the non-sensor specific features. \n",
    "        #Extract the initial guesses of azimuth, zenith and the number of clusters.    \n",
    "        az_t_pre = pre_feature.loc[pre_feature['event_id']==event_id]['az_t_pred']\n",
    "        ze_t_pre = pre_feature.loc[pre_feature['event_id']==event_id]['ze_t_pred']\n",
    "        num_clusters= pre_feature.loc[pre_feature['event_id']==event_id]['num_clusters']\n",
    "        \n",
    "        #Omit the sensor_id and flattern the sensor feature array. \n",
    "        flattened_pulse = (pulse_array_sensors[:, 1:]).flatten()\n",
    "        \n",
    "        #Concatenate the event features, \n",
    "        flattened_pulse = np.append(flattened_pulse,[az_t_pre,ze_t_pre,num_clusters])\n",
    "                \n",
    "        #Return our features and true azimuth and zenith as torch tensors\n",
    "        return (torch.from_numpy(flattened_pulse), \n",
    "                                 torch.from_numpy(meta_vals[-2:]))\n",
    "    \n",
    "        \n",
    "    #Function which computes the pulse with maximum charge of a given event and outputs its sensor_id, time and position\n",
    "    def loudest_bang(self, event):\n",
    "        charges=event.charge.values\n",
    "        sensors=event.sensor_id.values\n",
    "        times=event.time.values\n",
    "        i=charges.argmax(axis=0)\n",
    "        sen_max=sensors[i]\n",
    "        time_max=times[i]\n",
    "        xyz_from_id=self.id_to_xyz(sen_max)\n",
    "        max_pos=[xyz_from_id[0], xyz_from_id[1], xyz_from_id[2]]\n",
    "            \n",
    "        return [sen_max,time_max,max_pos]\n",
    "        \n",
    "        \n",
    "    #Computes xyz-coordinates based of sensor based on sensor_id   \n",
    "    def id_to_xyz(self, sen):\n",
    "        row = tuple(self.sensor_geom.loc[sen][1:4])\n",
    "        return row\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864253e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A simple fully connected neural network. \n",
    "We construct several layers, and use tanh activation \n",
    "to allow for +/- information to propagate. \n",
    "We then apply a final linear classifier. \n",
    "'''\n",
    "\n",
    "class NNpredictor(torch.nn.Module):\n",
    "    def __init__(self,  use_activation = True ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Set the Layers for the Neural Net\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        self.use_activation = use_activation\n",
    "        \n",
    "        self.layers.append(nn.Linear(dataset.num_features+3,\n",
    "            4000, dtype=float))\n",
    "        self.layers.append(nn.Linear(4000, 2000, dtype=float))\n",
    "        self.layers.append(nn.Linear(2000, 1000, dtype=float))\n",
    "        self.layers.append(nn.Linear(1000, 500, dtype=float))\n",
    "        self.layers.append(nn.Linear(500, 100, dtype=float))\n",
    "        self.layers.append(nn.Linear(100, 50, dtype=float))\n",
    "        self.layers.append(nn.Linear(50, 10, dtype=float))\n",
    "        self.classifier = (nn.Linear(10,2, dtype=float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x = x.to(device)\n",
    "        if(self.use_activation):\n",
    "            for layer in self.layers:\n",
    "                new_x = layer(new_x)\n",
    "                new_x= nn.Tanh()(new_x)\n",
    "                \n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                new_x = layer(new_x)\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "        return self.classifier(new_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d951c1-8b73-4614-bddd-bb98112c2209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define our custom loss class\n",
    "class custom_MAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_MAE, self).__init__();\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        loss_value = self.angular_dist_score(predictions, target).to(device)\n",
    "        return loss_value\n",
    "    \n",
    "    #This is the scoring metric provided by Kaggle\n",
    "    def angular_dist_score(self, predictions, true):\n",
    "        '''\n",
    "        calculate the MAE of the angular distance between two directions.\n",
    "        The two vectors are first converted to cartesian unit vectors,\n",
    "        and then their scalar product is computed, which is equal to\n",
    "        the cosine of the angle between the two vectors. The inverse \n",
    "        cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "    \n",
    "        az_true : float (or array thereof)\n",
    "            true azimuth value(s) in radian\n",
    "        zen_true : float (or array thereof)\n",
    "            true zenith value(s) in radian\n",
    "        az_pre : float (or array thereof)\n",
    "            predicted azimuth value(s) in radian\n",
    "        zen_pre : float (or array thereof)\n",
    "            predicted zenith value(s) in radian\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "    \n",
    "        dist : float\n",
    "            mean over the angular distance(s) in radian\n",
    "        '''\n",
    "    \n",
    "        az_true=true[:,0].to(device)\n",
    "        zen_true=true[:,1].to(device)\n",
    "        az_pred=predictions[:,0].to(device)\n",
    "        zen_pred=predictions[:,1].to(device)\n",
    "    \n",
    "        if not (torch.all(torch.isfinite(az_true)) and\n",
    "                torch.all(torch.isfinite(zen_true)) and\n",
    "                torch.all(torch.isfinite(az_pred)) and\n",
    "                torch.all(torch.isfinite(zen_pred))):\n",
    "            raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "        # pre-compute all sine and cosine values\n",
    "        sa1 = torch.sin(az_true).to(device)\n",
    "        ca1 = torch.cos(az_true).to(device)\n",
    "        sz1 = torch.sin(zen_true).to(device)\n",
    "        cz1 = torch.cos(zen_true).to(device)\n",
    "    \n",
    "        sa2 = torch.sin(az_pred).to(device)\n",
    "        ca2 = torch.cos(az_pred).to(device)\n",
    "        sz2 = torch.sin(zen_pred).to(device)\n",
    "        cz2 = torch.cos(zen_pred).to(device)\n",
    "    \n",
    "        # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "        scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "        # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
    "        # that might otherwise occure from the finite precision of the sine and cosine functions\n",
    "        scalar_prod =  torch.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "        # convert back to an angle (in radian)\n",
    "        return torch.mean(torch.abs(torch.arccos(scalar_prod))).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8ef4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the Training Loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch, lr, bs):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model=model.train()\n",
    "    loss_list=np.empty(0)\n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "        # Compute preiction and loss\n",
    "        pred = model(X).to(device)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Format training output\n",
    "        new_loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        loss_list=np.append(loss_list, loss.item())\n",
    "        if (batch % 100 == 0 ):\n",
    "            loss_list=loss_list[-100:]\n",
    "            loss=np.mean(loss_list)\n",
    "            print(f\"epoch: {epoch:>2d}, lr: {lr:>2f}, batch_size: {bs:>5d}, loss: {loss:>f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be48cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_loop(model, val_set):\n",
    "    #Load the validation data\n",
    "    validation_dataloader=DataLoader(val_set, batch_size=5, shuffle=False, num_workers=0, generator=torch.Generator(device=device))\n",
    "    \n",
    "    #initialize the loss and number of events\n",
    "    loss_total = 0\n",
    "    num = 0\n",
    "    \n",
    "    #Set the model to evaluate\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch, (X, y) in enumerate(validation_dataloader):\n",
    "\n",
    "            # Compute preiction and loss\n",
    "            pred = model(X)\n",
    "            loss_total += loss_fn.angular_dist_score(pred,y)\n",
    "            num +=1\n",
    "            mean=loss_total/num\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5a585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set epoch, batch size, learning rate, loss_fn. \n",
    "epoch=20\n",
    "batch_size=10\n",
    "learning_rate = 1e-5\n",
    "loss_fn = custom_MAE()\n",
    "\n",
    "#Set choice of training batch\n",
    "batch_id=10\n",
    "batch_path=batch_dir+\"batch_\"+str(batch_id)+\".parquet\"\n",
    "\n",
    "#Load dataset, setup data splits, and set manual seed\n",
    "dataset = NeutrinoDataset(batch_path, sensor_geom_path, batch_id, aux=False)\n",
    "sub_dataset=torch.utils.data.Subset(dataset, np.arange(16000))\n",
    "train=torch.utils.data.Subset(dataset, np.arange(12000))\n",
    "test=torch.utils.data.Subset(dataset, np.arange(12000, 16000))\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Initalize the NN, optimizer, dataloader. \n",
    "model = NNpredictor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a369b58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    model=train_loop(train_dataloader, model, loss_fn, optimizer, i, learning_rate, batch_size)\n",
    "    mean=validate_loop(model, test)\n",
    "    print(f\"Validation MAE: {mean:>5f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb57f4-23f8-4f5b-9055-e3c2c76d9cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
