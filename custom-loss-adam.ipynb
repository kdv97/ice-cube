{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2231cfc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "#Ben Directories\n",
    "sensor_geom_path='/opt/app/data/erdos-data/sensor_geometry.csv'\n",
    "batch_path='/opt/app/data/erdos-data/train/batch_104.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd97c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without considerations for the auxiliary label and no explicit features consisting of sensor geometry\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Class for a dataset generated from a dataframe and data from the sensor geometry file\n",
    "class NeutrinoDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.sensor_geom = pd.read_csv(sensor_geom_path)\n",
    "        self.vals_df = pd.read_parquet('batch_104_directions.parquet')\n",
    "        self.dataframe = pd.read_parquet(filename)\n",
    "        sensor_loc = np.array(sg.iloc[:])[:, 1:]\n",
    "        self.num_features = 5160*3\n",
    "        self.num_events = self.dataframe.index.nunique()\n",
    "        self.unique_indices = np.unique(self.dataframe.index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_events\n",
    "    \n",
    "    # Replaces sensor ID with sensor coordinates \n",
    "    def __getitem__(self, i):\n",
    "        df = self.dataframe\n",
    "        sg = self.sensor_geom\n",
    "        meta_vals = np.array(\n",
    "            self.vals_df.loc[self.vals_df['event_id'] == df.index[i]])[0].astype(float)\n",
    "        \n",
    "        pulse_array = np.array(df.loc[df.index[i]])\n",
    "        pulse_array_sensors = np.concatenate((np.expand_dims(np.arange(5160), axis=1), np.zeros([5160, 3])), 1)\n",
    "\n",
    "        for pulse in pulse_array:\n",
    "            if(pulse_array_sensors[pulse[0]][1] == 0):\n",
    "                pulse_array_sensors[pulse[0]][1] = pulse[1] - meta_vals[2] # first time\n",
    "            else:\n",
    "                # possible last time, will be the last time for the actual last one\n",
    "                pulse_array_sensors[pulse[0]][2] = pulse[1] - meta_vals[2]\n",
    "            # Add charge\n",
    "            pulse_array_sensors[pulse[0]][3] += pulse[2]\n",
    "        \n",
    "        flattened_pulse = (pulse_array_sensors[:, 1:]).flatten()\n",
    "        # print(flattened_pulse.shape)\n",
    "                \n",
    "        return (torch.from_numpy(flattened_pulse), \n",
    "                                 torch.from_numpy(meta_vals[-2:]))\n",
    "    \n",
    "    # Finds the first event with multiple pulses at the same sensors\n",
    "    # here we ask for at least num_min_total_repeats repetitions\n",
    "    def get_multi_pulse_event(self, num_min_total_repeats):\n",
    "        for i in range(self.num_events):\n",
    "            pulses = np.array(df.loc[unique_indices[i]])\n",
    "            if(pulses[:,0].shape[0] - np.unique(pulses[:,0]).shape[0] >= num_min_total_repeats):\n",
    "                return self.unique_indices[i]\n",
    "            \n",
    "    # Finds all events in a range with multiple pulses at the same sensors\n",
    "    # here we ask for at least num_min_total_repeats repetitions\n",
    "    def get_multi_pulse_events(self, num_min_total_repeats, start_index, end_index):\n",
    "        list_multi_pulse = []\n",
    "        for i in range(start_index, min(self.num_events, end_index)):\n",
    "            pulses = np.array(df.loc[unique_indices[i]])\n",
    "            if(pulses[:,0].shape[0] - np.unique(pulses[:,0]).shape[0] >= num_min_total_repeats):\n",
    "                list_multi_pulse.append(self.unique_indices[i])\n",
    "        return list_multi_pulse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95e6ec88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Checking torch device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "864253e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "# Set up Dataset and DataLoader, build NN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sg = pd.read_csv(sensor_geom_path)\n",
    "\n",
    "dataset = NeutrinoDataset(batch_path)\n",
    "\n",
    "\n",
    "class NNPredictor(torch.nn.Module):\n",
    "    def __init__(self, use_activation = True):\n",
    "        super().__init__()\n",
    "        # torch.manual_seed(1234)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        self.use_activation = use_activation\n",
    "        \n",
    "        self.layers.append(nn.Linear(dataset.num_features, 100))\n",
    "        self.layers.append(nn.Linear(100, 50))\n",
    "        self.layers.append(nn.Linear(50, 10))\n",
    "        self.classifier = (nn.Linear(10,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x = x\n",
    "        if(self.use_activation):\n",
    "            for layer in self.layers:\n",
    "                # print(layer, new_x.shape)\n",
    "                new_x = layer(new_x)\n",
    "                new_x = nn.ReLU()(new_x)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                new_x = layer(new_x)\n",
    "        \n",
    "        # Apply a final (linear) classifier.\n",
    "\n",
    "        return self.classifier(new_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4461540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = NNPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf0e8f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_104_vals_df = pd.read_parquet('batch_104_directions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9a687a-4fee-4f8a-a64c-49d647fdea72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def angular_dist_score(predictions, true):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse \n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    az_true=true[0]\n",
    "    zen_true=true[1]\n",
    "    az_pred=predictions[0]\n",
    "    zen_pred=predictions[1]\n",
    "    \n",
    "    if not (torch.all(torch.isfinite(az_true)) and\n",
    "            torch.all(torch.isfinite(zen_true)) and\n",
    "            torch.all(torch.isfinite(az_pred)) and\n",
    "            torch.all(torch.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "    # pre-compute all sine and cosine values\n",
    "    sa1 = torch.sin(az_true)\n",
    "    ca1 = torch.cos(az_true)\n",
    "    sz1 = torch.sin(zen_true)\n",
    "    cz1 = torch.cos(zen_true)\n",
    "    \n",
    "    sa2 = torch.sin(az_pred)\n",
    "    ca2 = torch.cos(az_pred)\n",
    "    sz2 = torch.sin(zen_pred)\n",
    "    cz2 = torch.cos(zen_pred)\n",
    "    \n",
    "    # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "    # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
    "    # that might otherwise occure from the finite precision of the sine and cosine functions\n",
    "    scalar_prod =  torch.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "    # convert back to an angle (in radian)\n",
    "    return torch.mean(torch.abs(torch.arccos(scalar_prod)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59d951c1-8b73-4614-bddd-bb98112c2209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class custom_MAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_MAE, self).__init__();\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        loss_value = angular_dist_score(predictions, target)\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfb8ef4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-9\n",
    "\n",
    "loss_fn = custom_MAE()\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(X)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ff187c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.379610  [   32/200000]\n",
      "loss: 0.946007  [   64/200000]\n",
      "loss: 1.204563  [   96/200000]\n",
      "loss: 1.899619  [  128/200000]\n",
      "loss: 0.929960  [  160/200000]\n",
      "loss: 1.147071  [  192/200000]\n",
      "loss: 1.094320  [  224/200000]\n",
      "loss: 1.214869  [  256/200000]\n",
      "loss: 0.516315  [  288/200000]\n",
      "loss: 0.596543  [  320/200000]\n",
      "loss: 1.074611  [  352/200000]\n",
      "loss: 1.633885  [  384/200000]\n",
      "loss: 1.306444  [  416/200000]\n",
      "loss: 1.687012  [  448/200000]\n",
      "loss: 0.748770  [  480/200000]\n",
      "loss: 0.917006  [  512/200000]\n",
      "loss: 2.011028  [  544/200000]\n",
      "loss: 1.468147  [  576/200000]\n",
      "loss: 1.345124  [  608/200000]\n",
      "loss: 1.712065  [  640/200000]\n",
      "loss: 1.556729  [  672/200000]\n",
      "loss: 1.574315  [  704/200000]\n",
      "loss: 1.436975  [  736/200000]\n",
      "loss: 0.772764  [  768/200000]\n",
      "loss: 2.050138  [  800/200000]\n",
      "loss: 1.372484  [  832/200000]\n",
      "loss: 1.205266  [  864/200000]\n",
      "loss: 0.612417  [  896/200000]\n",
      "loss: 1.559798  [  928/200000]\n",
      "loss: 0.974482  [  960/200000]\n",
      "loss: 0.610495  [  992/200000]\n",
      "loss: 1.331641  [ 1024/200000]\n",
      "loss: 0.779525  [ 1056/200000]\n",
      "loss: 0.518136  [ 1088/200000]\n",
      "loss: 1.716993  [ 1120/200000]\n",
      "loss: 0.963869  [ 1152/200000]\n",
      "loss: 0.590218  [ 1184/200000]\n",
      "loss: 2.439885  [ 1216/200000]\n",
      "loss: 1.210462  [ 1248/200000]\n",
      "loss: 1.119681  [ 1280/200000]\n",
      "loss: 1.058955  [ 1312/200000]\n",
      "loss: 0.669274  [ 1344/200000]\n",
      "loss: 1.234231  [ 1376/200000]\n",
      "loss: 2.727569  [ 1408/200000]\n",
      "loss: 1.139836  [ 1440/200000]\n",
      "loss: 1.538998  [ 1472/200000]\n",
      "loss: 1.194312  [ 1504/200000]\n",
      "loss: 2.205994  [ 1536/200000]\n",
      "loss: 1.598237  [ 1568/200000]\n",
      "loss: 2.614428  [ 1600/200000]\n",
      "loss: 1.924002  [ 1632/200000]\n",
      "loss: 2.244854  [ 1664/200000]\n",
      "loss: 1.632119  [ 1696/200000]\n",
      "loss: 2.163091  [ 1728/200000]\n",
      "loss: 1.897063  [ 1760/200000]\n",
      "loss: 1.614306  [ 1792/200000]\n",
      "loss: 1.470883  [ 1824/200000]\n",
      "loss: 1.316558  [ 1856/200000]\n",
      "loss: 0.556368  [ 1888/200000]\n",
      "loss: 2.491611  [ 1920/200000]\n",
      "loss: 0.718026  [ 1952/200000]\n",
      "loss: 2.069571  [ 1984/200000]\n",
      "loss: 2.646618  [ 2016/200000]\n",
      "loss: 0.937783  [ 2048/200000]\n",
      "loss: 1.072405  [ 2080/200000]\n",
      "loss: 2.424683  [ 2112/200000]\n",
      "loss: 1.925012  [ 2144/200000]\n",
      "loss: 2.217288  [ 2176/200000]\n",
      "loss: 0.734305  [ 2208/200000]\n",
      "loss: 1.060010  [ 2240/200000]\n",
      "loss: 0.664058  [ 2272/200000]\n",
      "loss: 2.489251  [ 2304/200000]\n",
      "loss: 0.260881  [ 2336/200000]\n",
      "loss: 1.479897  [ 2368/200000]\n",
      "loss: 1.816273  [ 2400/200000]\n",
      "loss: 2.313164  [ 2432/200000]\n",
      "loss: 2.304048  [ 2464/200000]\n",
      "loss: 0.397528  [ 2496/200000]\n",
      "loss: 0.748030  [ 2528/200000]\n",
      "loss: 2.046872  [ 2560/200000]\n",
      "loss: 0.296681  [ 2592/200000]\n",
      "loss: 0.831003  [ 2624/200000]\n",
      "loss: 0.405405  [ 2656/200000]\n",
      "loss: 0.962305  [ 2688/200000]\n",
      "loss: 2.067294  [ 2720/200000]\n",
      "loss: 2.512338  [ 2752/200000]\n",
      "loss: 1.453886  [ 2784/200000]\n",
      "loss: 0.963858  [ 2816/200000]\n",
      "loss: 0.506736  [ 2848/200000]\n",
      "loss: 1.613339  [ 2880/200000]\n",
      "loss: 1.246642  [ 2912/200000]\n",
      "loss: 0.517928  [ 2944/200000]\n",
      "loss: 0.924823  [ 2976/200000]\n",
      "loss: 1.545766  [ 3008/200000]\n",
      "loss: 1.713814  [ 3040/200000]\n",
      "loss: 2.321061  [ 3072/200000]\n",
      "loss: 1.637315  [ 3104/200000]\n",
      "loss: 1.341094  [ 3136/200000]\n",
      "loss: 0.671426  [ 3168/200000]\n",
      "loss: 1.579628  [ 3200/200000]\n",
      "loss: 0.404196  [ 3232/200000]\n",
      "loss: 1.769648  [ 3264/200000]\n",
      "loss: 2.166088  [ 3296/200000]\n",
      "loss: 1.053004  [ 3328/200000]\n",
      "loss: 1.163001  [ 3360/200000]\n",
      "loss: 0.762191  [ 3392/200000]\n",
      "loss: 0.829583  [ 3424/200000]\n",
      "loss: 1.114157  [ 3456/200000]\n",
      "loss: 0.915583  [ 3488/200000]\n",
      "loss: 1.223479  [ 3520/200000]\n",
      "loss: 1.370851  [ 3552/200000]\n",
      "loss: 1.109093  [ 3584/200000]\n",
      "loss: 0.613761  [ 3616/200000]\n",
      "loss: 1.961937  [ 3648/200000]\n",
      "loss: 1.120528  [ 3680/200000]\n",
      "loss: 1.482786  [ 3712/200000]\n",
      "loss: 1.805027  [ 3744/200000]\n",
      "loss: 1.681386  [ 3776/200000]\n",
      "loss: 1.714911  [ 3808/200000]\n",
      "loss: 0.579190  [ 3840/200000]\n",
      "loss: 0.611717  [ 3872/200000]\n",
      "loss: 2.033668  [ 3904/200000]\n",
      "loss: 0.255975  [ 3936/200000]\n",
      "loss: 1.567139  [ 3968/200000]\n",
      "loss: 0.755882  [ 4000/200000]\n",
      "loss: 0.996097  [ 4032/200000]\n",
      "loss: 0.948588  [ 4064/200000]\n",
      "loss: 2.133873  [ 4096/200000]\n",
      "loss: 1.161451  [ 4128/200000]\n",
      "loss: 1.519125  [ 4160/200000]\n",
      "loss: 0.792140  [ 4192/200000]\n",
      "loss: 2.342412  [ 4224/200000]\n",
      "loss: 1.896728  [ 4256/200000]\n",
      "loss: 1.678593  [ 4288/200000]\n",
      "loss: 2.140961  [ 4320/200000]\n",
      "loss: 1.660004  [ 4352/200000]\n",
      "loss: 2.409865  [ 4384/200000]\n",
      "loss: 1.798521  [ 4416/200000]\n",
      "loss: 1.617505  [ 4448/200000]\n",
      "loss: 1.356630  [ 4480/200000]\n",
      "loss: 1.196215  [ 4512/200000]\n",
      "loss: 1.559991  [ 4544/200000]\n",
      "loss: 1.172384  [ 4576/200000]\n",
      "loss: 1.299520  [ 4608/200000]\n",
      "loss: 2.449280  [ 4640/200000]\n",
      "loss: 2.523966  [ 4672/200000]\n",
      "loss: 0.671891  [ 4704/200000]\n",
      "loss: 1.918933  [ 4736/200000]\n",
      "loss: 0.922439  [ 4768/200000]\n",
      "loss: 0.817677  [ 4800/200000]\n",
      "loss: 1.468770  [ 4832/200000]\n",
      "loss: 1.112064  [ 4864/200000]\n",
      "loss: 1.013089  [ 4896/200000]\n",
      "loss: 1.860994  [ 4928/200000]\n",
      "loss: 1.424193  [ 4960/200000]\n",
      "loss: 0.065735  [ 4992/200000]\n",
      "loss: 1.764143  [ 5024/200000]\n",
      "loss: 2.222798  [ 5056/200000]\n",
      "loss: 1.599906  [ 5088/200000]\n",
      "loss: 2.216158  [ 5120/200000]\n",
      "loss: 0.473874  [ 5152/200000]\n",
      "loss: 1.196194  [ 5184/200000]\n",
      "loss: 1.323981  [ 5216/200000]\n",
      "loss: 1.508292  [ 5248/200000]\n",
      "loss: 1.019996  [ 5280/200000]\n",
      "loss: 2.249819  [ 5312/200000]\n",
      "loss: 0.858894  [ 5344/200000]\n",
      "loss: 1.177699  [ 5376/200000]\n",
      "loss: 2.211855  [ 5408/200000]\n",
      "loss: 1.290735  [ 5440/200000]\n",
      "loss: 1.040495  [ 5472/200000]\n",
      "loss: 1.177511  [ 5504/200000]\n",
      "loss: 2.000956  [ 5536/200000]\n",
      "loss: 0.926089  [ 5568/200000]\n",
      "loss: 1.489777  [ 5600/200000]\n",
      "loss: 0.065809  [ 5632/200000]\n",
      "loss: 1.322352  [ 5664/200000]\n",
      "loss: 1.015607  [ 5696/200000]\n",
      "loss: 1.792477  [ 5728/200000]\n",
      "loss: 0.665920  [ 5760/200000]\n",
      "loss: 0.424649  [ 5792/200000]\n",
      "loss: 2.100893  [ 5824/200000]\n",
      "loss: 1.653094  [ 5856/200000]\n",
      "loss: 2.127260  [ 5888/200000]\n",
      "loss: 0.423192  [ 5920/200000]\n",
      "loss: 2.092920  [ 5952/200000]\n",
      "loss: 2.073529  [ 5984/200000]\n",
      "loss: 2.086818  [ 6016/200000]\n",
      "loss: 2.563924  [ 6048/200000]\n",
      "loss: 0.976614  [ 6080/200000]\n",
      "loss: 0.717311  [ 6112/200000]\n",
      "loss: 1.077199  [ 6144/200000]\n",
      "loss: 0.448686  [ 6176/200000]\n",
      "loss: 1.038211  [ 6208/200000]\n",
      "loss: 1.185789  [ 6240/200000]\n",
      "loss: 2.204221  [ 6272/200000]\n",
      "loss: 1.805454  [ 6304/200000]\n",
      "loss: 1.194201  [ 6336/200000]\n",
      "loss: 2.442143  [ 6368/200000]\n",
      "loss: 1.520744  [ 6400/200000]\n",
      "loss: 1.387137  [ 6432/200000]\n",
      "loss: 1.407013  [ 6464/200000]\n",
      "loss: 0.730708  [ 6496/200000]\n",
      "loss: 1.167593  [ 6528/200000]\n",
      "loss: 1.022258  [ 6560/200000]\n",
      "loss: 1.543170  [ 6592/200000]\n",
      "loss: 1.535115  [ 6624/200000]\n",
      "loss: 2.687003  [ 6656/200000]\n",
      "loss: 1.987895  [ 6688/200000]\n",
      "loss: 1.932471  [ 6720/200000]\n",
      "loss: 0.999248  [ 6752/200000]\n",
      "loss: 1.652543  [ 6784/200000]\n",
      "loss: 0.790901  [ 6816/200000]\n",
      "loss: 0.806079  [ 6848/200000]\n",
      "loss: 1.512035  [ 6880/200000]\n",
      "loss: 1.483172  [ 6912/200000]\n",
      "loss: 1.121989  [ 6944/200000]\n",
      "loss: 2.084512  [ 6976/200000]\n",
      "loss: 1.549433  [ 7008/200000]\n",
      "loss: 0.566333  [ 7040/200000]\n",
      "loss: 1.401178  [ 7072/200000]\n",
      "loss: 1.032484  [ 7104/200000]\n",
      "loss: 2.461818  [ 7136/200000]\n",
      "loss: 1.089306  [ 7168/200000]\n",
      "loss: 1.466064  [ 7200/200000]\n",
      "loss: 1.545107  [ 7232/200000]\n",
      "loss: 1.631284  [ 7264/200000]\n",
      "loss: 1.888555  [ 7296/200000]\n",
      "loss: 1.625399  [ 7328/200000]\n",
      "loss: 1.691205  [ 7360/200000]\n",
      "loss: 1.966499  [ 7392/200000]\n",
      "loss: 2.767108  [ 7424/200000]\n",
      "loss: 1.876540  [ 7456/200000]\n",
      "loss: 1.267381  [ 7488/200000]\n",
      "loss: 1.012866  [ 7520/200000]\n",
      "loss: 1.519528  [ 7552/200000]\n",
      "loss: 2.280490  [ 7584/200000]\n",
      "loss: 0.700549  [ 7616/200000]\n",
      "loss: 1.373433  [ 7648/200000]\n",
      "loss: 1.528272  [ 7680/200000]\n",
      "loss: 1.097298  [ 7712/200000]\n",
      "loss: 1.002177  [ 7744/200000]\n",
      "loss: 1.479094  [ 7776/200000]\n",
      "loss: 0.894976  [ 7808/200000]\n",
      "loss: 2.345157  [ 7840/200000]\n",
      "loss: 0.994756  [ 7872/200000]\n",
      "loss: 0.685801  [ 7904/200000]\n",
      "loss: 2.160801  [ 7936/200000]\n",
      "loss: 1.256071  [ 7968/200000]\n",
      "loss: 2.384186  [ 8000/200000]\n",
      "loss: 1.724210  [ 8032/200000]\n",
      "loss: 0.439682  [ 8064/200000]\n",
      "loss: 1.981179  [ 8096/200000]\n",
      "loss: 0.635848  [ 8128/200000]\n",
      "loss: 1.146608  [ 8160/200000]\n",
      "loss: 1.145796  [ 8192/200000]\n",
      "loss: 1.507054  [ 8224/200000]\n",
      "loss: 1.339208  [ 8256/200000]\n",
      "loss: 1.387401  [ 8288/200000]\n",
      "loss: 0.660097  [ 8320/200000]\n",
      "loss: 1.648903  [ 8352/200000]\n",
      "loss: 0.884730  [ 8384/200000]\n",
      "loss: 0.814944  [ 8416/200000]\n",
      "loss: 0.448435  [ 8448/200000]\n",
      "loss: 2.136133  [ 8480/200000]\n",
      "loss: 1.216623  [ 8512/200000]\n",
      "loss: 1.109764  [ 8544/200000]\n",
      "loss: 0.198641  [ 8576/200000]\n",
      "loss: 1.680859  [ 8608/200000]\n",
      "loss: 0.993319  [ 8640/200000]\n",
      "loss: 1.878292  [ 8672/200000]\n",
      "loss: 1.509424  [ 8704/200000]\n",
      "loss: 2.617925  [ 8736/200000]\n",
      "loss: 1.528478  [ 8768/200000]\n",
      "loss: 1.036834  [ 8800/200000]\n",
      "loss: 1.710447  [ 8832/200000]\n",
      "loss: 0.729844  [ 8864/200000]\n",
      "loss: 1.613630  [ 8896/200000]\n",
      "loss: 0.816816  [ 8928/200000]\n",
      "loss: 0.686876  [ 8960/200000]\n",
      "loss: 2.032215  [ 8992/200000]\n",
      "loss: 2.068731  [ 9024/200000]\n",
      "loss: 2.598779  [ 9056/200000]\n",
      "loss: 1.170761  [ 9088/200000]\n",
      "loss: 2.717900  [ 9120/200000]\n",
      "loss: 1.007103  [ 9152/200000]\n",
      "loss: 2.026104  [ 9184/200000]\n",
      "loss: 0.385518  [ 9216/200000]\n",
      "loss: 0.499027  [ 9248/200000]\n",
      "loss: 0.934248  [ 9280/200000]\n",
      "loss: 0.612430  [ 9312/200000]\n",
      "loss: 1.256320  [ 9344/200000]\n",
      "loss: 1.528668  [ 9376/200000]\n",
      "loss: 0.934631  [ 9408/200000]\n",
      "loss: 0.954556  [ 9440/200000]\n",
      "loss: 1.483466  [ 9472/200000]\n",
      "loss: 1.108613  [ 9504/200000]\n",
      "loss: 0.751041  [ 9536/200000]\n",
      "loss: 1.737610  [ 9568/200000]\n",
      "loss: 2.411917  [ 9600/200000]\n",
      "loss: 1.306489  [ 9632/200000]\n",
      "loss: 0.714453  [ 9664/200000]\n",
      "loss: 1.327066  [ 9696/200000]\n",
      "loss: 1.630976  [ 9728/200000]\n",
      "loss: 0.838991  [ 9760/200000]\n",
      "loss: 0.698148  [ 9792/200000]\n",
      "loss: 0.895759  [ 9824/200000]\n",
      "loss: 0.851220  [ 9856/200000]\n",
      "loss: 1.941746  [ 9888/200000]\n",
      "loss: 1.934329  [ 9920/200000]\n",
      "loss: 0.725137  [ 9952/200000]\n",
      "loss: 1.374886  [ 9984/200000]\n",
      "loss: 1.415124  [10016/200000]\n",
      "loss: 2.083975  [10048/200000]\n",
      "loss: 1.175681  [10080/200000]\n",
      "loss: 1.630604  [10112/200000]\n",
      "loss: 1.463262  [10144/200000]\n",
      "loss: 2.240332  [10176/200000]\n",
      "loss: 0.795056  [10208/200000]\n",
      "loss: 0.524838  [10240/200000]\n",
      "loss: 1.188800  [10272/200000]\n",
      "loss: 1.370106  [10304/200000]\n",
      "loss: 1.369843  [10336/200000]\n",
      "loss: 0.964780  [10368/200000]\n",
      "loss: 1.660894  [10400/200000]\n",
      "loss: 2.156634  [10432/200000]\n",
      "loss: 0.974445  [10464/200000]\n",
      "loss: 0.822948  [10496/200000]\n",
      "loss: 1.181724  [10528/200000]\n",
      "loss: 1.793908  [10560/200000]\n",
      "loss: 1.594706  [10592/200000]\n",
      "loss: 2.667802  [10624/200000]\n",
      "loss: 1.152753  [10656/200000]\n",
      "loss: 1.058341  [10688/200000]\n",
      "loss: 0.552044  [10720/200000]\n",
      "loss: 1.552543  [10752/200000]\n",
      "loss: 1.884775  [10784/200000]\n",
      "loss: 1.945545  [10816/200000]\n",
      "loss: 1.067511  [10848/200000]\n",
      "loss: 2.223574  [10880/200000]\n",
      "loss: 0.359978  [10912/200000]\n",
      "loss: 0.580535  [10944/200000]\n",
      "loss: 1.265888  [10976/200000]\n",
      "loss: 0.411442  [11008/200000]\n",
      "loss: 0.706986  [11040/200000]\n",
      "loss: 2.149531  [11072/200000]\n",
      "loss: 0.980068  [11104/200000]\n",
      "loss: 1.437617  [11136/200000]\n",
      "loss: 1.871419  [11168/200000]\n",
      "loss: 1.271705  [11200/200000]\n",
      "loss: 1.055545  [11232/200000]\n",
      "loss: 1.360709  [11264/200000]\n",
      "loss: 0.400383  [11296/200000]\n",
      "loss: 0.875636  [11328/200000]\n",
      "loss: 1.127794  [11360/200000]\n",
      "loss: 2.296655  [11392/200000]\n",
      "loss: 1.306351  [11424/200000]\n",
      "loss: 0.610412  [11456/200000]\n",
      "loss: 0.729338  [11488/200000]\n",
      "loss: 1.219948  [11520/200000]\n",
      "loss: 0.629928  [11552/200000]\n",
      "loss: 1.699628  [11584/200000]\n",
      "loss: 0.119649  [11616/200000]\n",
      "loss: 2.690057  [11648/200000]\n",
      "loss: 1.032517  [11680/200000]\n",
      "loss: 2.745856  [11712/200000]\n",
      "loss: 1.233450  [11744/200000]\n",
      "loss: 0.795550  [11776/200000]\n",
      "loss: 1.378228  [11808/200000]\n",
      "loss: 1.808869  [11840/200000]\n",
      "loss: 2.127005  [11872/200000]\n",
      "loss: 1.157643  [11904/200000]\n",
      "loss: 0.109019  [11936/200000]\n",
      "loss: 1.176774  [11968/200000]\n",
      "loss: 0.554447  [12000/200000]\n",
      "loss: 1.131144  [12032/200000]\n",
      "loss: 2.293656  [12064/200000]\n",
      "loss: 1.549045  [12096/200000]\n",
      "loss: 1.648467  [12128/200000]\n",
      "loss: 1.024304  [12160/200000]\n",
      "loss: 1.418288  [12192/200000]\n",
      "loss: 2.760845  [12224/200000]\n",
      "loss: 1.301997  [12256/200000]\n",
      "loss: 0.853589  [12288/200000]\n",
      "loss: 0.304795  [12320/200000]\n",
      "loss: 0.944925  [12352/200000]\n",
      "loss: 1.003256  [12384/200000]\n",
      "loss: 0.653643  [12416/200000]\n",
      "loss: 0.675301  [12448/200000]\n",
      "loss: 1.298296  [12480/200000]\n",
      "loss: 2.344065  [12512/200000]\n",
      "loss: 0.636511  [12544/200000]\n",
      "loss: 0.865957  [12576/200000]\n",
      "loss: 0.533910  [12608/200000]\n",
      "loss: 2.270667  [12640/200000]\n",
      "loss: 1.892578  [12672/200000]\n",
      "loss: 1.522684  [12704/200000]\n",
      "loss: 2.058971  [12736/200000]\n",
      "loss: 0.856807  [12768/200000]\n",
      "loss: 1.429097  [12800/200000]\n",
      "loss: 1.122317  [12832/200000]\n",
      "loss: 1.234393  [12864/200000]\n",
      "loss: 2.372251  [12896/200000]\n",
      "loss: 2.505872  [12928/200000]\n",
      "loss: 1.869782  [12960/200000]\n",
      "loss: 2.098437  [12992/200000]\n",
      "loss: 1.085772  [13024/200000]\n",
      "loss: 2.490667  [13056/200000]\n",
      "loss: 1.687450  [13088/200000]\n",
      "loss: 1.342810  [13120/200000]\n",
      "loss: 0.722825  [13152/200000]\n",
      "loss: 2.572282  [13184/200000]\n",
      "loss: 1.639084  [13216/200000]\n",
      "loss: 1.156925  [13248/200000]\n",
      "loss: 1.581381  [13280/200000]\n",
      "loss: 1.237494  [13312/200000]\n",
      "loss: 0.489696  [13344/200000]\n",
      "loss: 0.388240  [13376/200000]\n",
      "loss: 1.358042  [13408/200000]\n",
      "loss: 0.687789  [13440/200000]\n",
      "loss: 0.562858  [13472/200000]\n",
      "loss: 0.526026  [13504/200000]\n",
      "loss: 1.000219  [13536/200000]\n",
      "loss: 1.271989  [13568/200000]\n",
      "loss: 0.471890  [13600/200000]\n",
      "loss: 1.162144  [13632/200000]\n",
      "loss: 1.498700  [13664/200000]\n",
      "loss: 2.073090  [13696/200000]\n",
      "loss: 1.073172  [13728/200000]\n",
      "loss: 1.120328  [13760/200000]\n",
      "loss: 0.173754  [13792/200000]\n",
      "loss: 1.923058  [13824/200000]\n",
      "loss: 1.896186  [13856/200000]\n",
      "loss: 1.152680  [13888/200000]\n",
      "loss: 1.738699  [13920/200000]\n",
      "loss: 2.741877  [13952/200000]\n",
      "loss: 1.972564  [13984/200000]\n",
      "loss: 1.619433  [14016/200000]\n",
      "loss: 1.823606  [14048/200000]\n",
      "loss: 0.411607  [14080/200000]\n",
      "loss: 1.144823  [14112/200000]\n",
      "loss: 1.692321  [14144/200000]\n",
      "loss: 1.350160  [14176/200000]\n",
      "loss: 0.526603  [14208/200000]\n",
      "loss: 1.015245  [14240/200000]\n",
      "loss: 1.142993  [14272/200000]\n",
      "loss: 0.692212  [14304/200000]\n",
      "loss: 1.073492  [14336/200000]\n",
      "loss: 1.113079  [14368/200000]\n",
      "loss: 1.464817  [14400/200000]\n",
      "loss: 0.352618  [14432/200000]\n",
      "loss: 0.608249  [14464/200000]\n",
      "loss: 1.368036  [14496/200000]\n",
      "loss: 2.234892  [14528/200000]\n",
      "loss: 1.127806  [14560/200000]\n",
      "loss: 2.074205  [14592/200000]\n",
      "loss: 1.087229  [14624/200000]\n",
      "loss: 1.288377  [14656/200000]\n",
      "loss: 1.243057  [14688/200000]\n",
      "loss: 2.493822  [14720/200000]\n",
      "loss: 0.609527  [14752/200000]\n",
      "loss: 0.649671  [14784/200000]\n",
      "loss: 0.484559  [14816/200000]\n",
      "loss: 0.342209  [14848/200000]\n",
      "loss: 0.488925  [14880/200000]\n",
      "loss: 0.681991  [14912/200000]\n",
      "loss: 1.165941  [14944/200000]\n",
      "loss: 0.901710  [14976/200000]\n",
      "loss: 0.975141  [15008/200000]\n",
      "loss: 1.028253  [15040/200000]\n",
      "loss: 1.133800  [15072/200000]\n",
      "loss: 0.309506  [15104/200000]\n",
      "loss: 0.986145  [15136/200000]\n",
      "loss: 1.041262  [15168/200000]\n",
      "loss: 0.305995  [15200/200000]\n",
      "loss: 1.143838  [15232/200000]\n",
      "loss: 0.715455  [15264/200000]\n",
      "loss: 1.401507  [15296/200000]\n",
      "loss: 1.307027  [15328/200000]\n",
      "loss: 1.710605  [15360/200000]\n",
      "loss: 1.156673  [15392/200000]\n",
      "loss: 2.570497  [15424/200000]\n",
      "loss: 0.754163  [15456/200000]\n",
      "loss: 1.001783  [15488/200000]\n",
      "loss: 0.912479  [15520/200000]\n",
      "loss: 1.722320  [15552/200000]\n",
      "loss: 0.703910  [15584/200000]\n",
      "loss: 1.009788  [15616/200000]\n",
      "loss: 0.625723  [15648/200000]\n",
      "loss: 2.610089  [15680/200000]\n",
      "loss: 0.741540  [15712/200000]\n",
      "loss: 1.571432  [15744/200000]\n",
      "loss: 0.673688  [15776/200000]\n",
      "loss: 1.715546  [15808/200000]\n",
      "loss: 1.021760  [15840/200000]\n",
      "loss: 1.044169  [15872/200000]\n",
      "loss: 1.303670  [15904/200000]\n",
      "loss: 1.432337  [15936/200000]\n",
      "loss: 2.528629  [15968/200000]\n",
      "loss: 2.525646  [16000/200000]\n",
      "loss: 0.862914  [16032/200000]\n",
      "loss: 1.974077  [16064/200000]\n",
      "loss: 0.760743  [16096/200000]\n",
      "loss: 1.568117  [16128/200000]\n",
      "loss: 2.545145  [16160/200000]\n",
      "loss: 1.353818  [16192/200000]\n",
      "loss: 1.345928  [16224/200000]\n",
      "loss: 0.972559  [16256/200000]\n",
      "loss: 0.741677  [16288/200000]\n",
      "loss: 1.769252  [16320/200000]\n",
      "loss: 2.098111  [16352/200000]\n",
      "loss: 2.678522  [16384/200000]\n",
      "loss: 1.253617  [16416/200000]\n",
      "loss: 1.212488  [16448/200000]\n",
      "loss: 1.670151  [16480/200000]\n",
      "loss: 0.286332  [16512/200000]\n",
      "loss: 1.542945  [16544/200000]\n",
      "loss: 0.516789  [16576/200000]\n",
      "loss: 1.995890  [16608/200000]\n",
      "loss: 1.039449  [16640/200000]\n",
      "loss: 0.770381  [16672/200000]\n",
      "loss: 2.124347  [16704/200000]\n",
      "loss: 0.505648  [16736/200000]\n",
      "loss: 0.570286  [16768/200000]\n",
      "loss: 1.113041  [16800/200000]\n",
      "loss: 2.092865  [16832/200000]\n",
      "loss: 0.566058  [16864/200000]\n",
      "loss: 2.331497  [16896/200000]\n",
      "loss: 1.468622  [16928/200000]\n",
      "loss: 1.371619  [16960/200000]\n",
      "loss: 1.408149  [16992/200000]\n",
      "loss: 0.915665  [17024/200000]\n",
      "loss: 2.104759  [17056/200000]\n",
      "loss: 1.183896  [17088/200000]\n",
      "loss: 0.194751  [17120/200000]\n",
      "loss: 1.360261  [17152/200000]\n",
      "loss: 1.040902  [17184/200000]\n",
      "loss: 1.115000  [17216/200000]\n",
      "loss: 1.013527  [17248/200000]\n",
      "loss: 0.900152  [17280/200000]\n",
      "loss: 0.965682  [17312/200000]\n",
      "loss: 1.560953  [17344/200000]\n",
      "loss: 1.357857  [17376/200000]\n",
      "loss: 1.239952  [17408/200000]\n",
      "loss: 0.178788  [17440/200000]\n",
      "loss: 1.087606  [17472/200000]\n",
      "loss: 1.286290  [17504/200000]\n",
      "loss: 1.458194  [17536/200000]\n",
      "loss: 1.032813  [17568/200000]\n",
      "loss: 1.200966  [17600/200000]\n",
      "loss: 1.270262  [17632/200000]\n",
      "loss: 2.196097  [17664/200000]\n",
      "loss: 2.470875  [17696/200000]\n",
      "loss: 2.383939  [17728/200000]\n",
      "loss: 0.221002  [17760/200000]\n",
      "loss: 1.664477  [17792/200000]\n",
      "loss: 1.787502  [17824/200000]\n",
      "loss: 0.430457  [17856/200000]\n",
      "loss: 1.381977  [17888/200000]\n",
      "loss: 0.938703  [17920/200000]\n",
      "loss: 0.522019  [17952/200000]\n",
      "loss: 1.611332  [17984/200000]\n",
      "loss: 1.536291  [18016/200000]\n",
      "loss: 0.978106  [18048/200000]\n",
      "loss: 1.334988  [18080/200000]\n",
      "loss: 1.172781  [18112/200000]\n",
      "loss: 1.249302  [18144/200000]\n",
      "loss: 0.457042  [18176/200000]\n",
      "loss: 1.463235  [18208/200000]\n",
      "loss: 0.985853  [18240/200000]\n",
      "loss: 1.697089  [18272/200000]\n",
      "loss: 0.890363  [18304/200000]\n",
      "loss: 0.432386  [18336/200000]\n",
      "loss: 1.285356  [18368/200000]\n",
      "loss: 0.849301  [18400/200000]\n",
      "loss: 1.174394  [18432/200000]\n",
      "loss: 1.147515  [18464/200000]\n",
      "loss: 1.861146  [18496/200000]\n",
      "loss: 1.099998  [18528/200000]\n",
      "loss: 1.696756  [18560/200000]\n",
      "loss: 0.847658  [18592/200000]\n",
      "loss: 1.986384  [18624/200000]\n",
      "loss: 0.484377  [18656/200000]\n",
      "loss: 1.645975  [18688/200000]\n",
      "loss: 1.898999  [18720/200000]\n",
      "loss: 0.143725  [18752/200000]\n",
      "loss: 0.252914  [18784/200000]\n",
      "loss: 1.298483  [18816/200000]\n",
      "loss: 1.541200  [18848/200000]\n",
      "loss: 1.261390  [18880/200000]\n",
      "loss: 1.034742  [18912/200000]\n",
      "loss: 0.613218  [18944/200000]\n",
      "loss: 1.362937  [18976/200000]\n",
      "loss: 0.375390  [19008/200000]\n",
      "loss: 1.824364  [19040/200000]\n",
      "loss: 1.323082  [19072/200000]\n",
      "loss: 0.496078  [19104/200000]\n",
      "loss: 1.475098  [19136/200000]\n",
      "loss: 1.342223  [19168/200000]\n",
      "loss: 0.172906  [19200/200000]\n",
      "loss: 0.135382  [19232/200000]\n",
      "loss: 1.143955  [19264/200000]\n",
      "loss: 0.486582  [19296/200000]\n",
      "loss: 0.194743  [19328/200000]\n",
      "loss: 1.562326  [19360/200000]\n",
      "loss: 0.989903  [19392/200000]\n",
      "loss: 1.356264  [19424/200000]\n",
      "loss: 0.980683  [19456/200000]\n",
      "loss: 1.297729  [19488/200000]\n",
      "loss: 1.242735  [19520/200000]\n",
      "loss: 0.826381  [19552/200000]\n",
      "loss: 1.610575  [19584/200000]\n",
      "loss: 0.406382  [19616/200000]\n",
      "loss: 1.438524  [19648/200000]\n",
      "loss: 0.507511  [19680/200000]\n",
      "loss: 0.713756  [19712/200000]\n",
      "loss: 0.916961  [19744/200000]\n",
      "loss: 0.691113  [19776/200000]\n",
      "loss: 1.152245  [19808/200000]\n",
      "loss: 1.964333  [19840/200000]\n",
      "loss: 2.440506  [19872/200000]\n",
      "loss: 1.204031  [19904/200000]\n",
      "loss: 0.687927  [19936/200000]\n",
      "loss: 1.354945  [19968/200000]\n",
      "loss: 1.626744  [20000/200000]\n",
      "loss: 0.840665  [20032/200000]\n",
      "loss: 0.530103  [20064/200000]\n",
      "loss: 0.525811  [20096/200000]\n",
      "loss: 2.171629  [20128/200000]\n",
      "loss: 1.870545  [20160/200000]\n",
      "loss: 2.272465  [20192/200000]\n",
      "loss: 0.340926  [20224/200000]\n",
      "loss: 1.522078  [20256/200000]\n",
      "loss: 0.893244  [20288/200000]\n",
      "loss: 1.422605  [20320/200000]\n",
      "loss: 0.333497  [20352/200000]\n",
      "loss: 1.111272  [20384/200000]\n",
      "loss: 0.733546  [20416/200000]\n",
      "loss: 2.665471  [20448/200000]\n",
      "loss: 0.872162  [20480/200000]\n",
      "loss: 1.770703  [20512/200000]\n",
      "loss: 1.675738  [20544/200000]\n",
      "loss: 1.069700  [20576/200000]\n",
      "loss: 1.011051  [20608/200000]\n",
      "loss: 0.856114  [20640/200000]\n",
      "loss: 0.448221  [20672/200000]\n",
      "loss: 1.390079  [20704/200000]\n",
      "loss: 2.392933  [20736/200000]\n",
      "loss: 1.128748  [20768/200000]\n",
      "loss: 1.377695  [20800/200000]\n",
      "loss: 1.508382  [20832/200000]\n",
      "loss: 2.014432  [20864/200000]\n",
      "loss: 0.221705  [20896/200000]\n",
      "loss: 1.436183  [20928/200000]\n",
      "loss: 1.219789  [20960/200000]\n",
      "loss: 1.277693  [20992/200000]\n",
      "loss: 1.194218  [21024/200000]\n",
      "loss: 1.965619  [21056/200000]\n",
      "loss: 1.735807  [21088/200000]\n",
      "loss: 1.480144  [21120/200000]\n",
      "loss: 0.105728  [21152/200000]\n",
      "loss: 1.140833  [21184/200000]\n",
      "loss: 1.687869  [21216/200000]\n",
      "loss: 1.259914  [21248/200000]\n",
      "loss: 0.635616  [21280/200000]\n",
      "loss: 1.297475  [21312/200000]\n",
      "loss: 2.033845  [21344/200000]\n",
      "loss: 0.077820  [21376/200000]\n",
      "loss: 0.996683  [21408/200000]\n",
      "loss: 0.732057  [21440/200000]\n",
      "loss: 0.602479  [21472/200000]\n",
      "loss: 1.439001  [21504/200000]\n",
      "loss: 1.214727  [21536/200000]\n",
      "loss: 0.609401  [21568/200000]\n",
      "loss: 1.667202  [21600/200000]\n",
      "loss: 0.837358  [21632/200000]\n",
      "loss: 1.376894  [21664/200000]\n",
      "loss: 1.366369  [21696/200000]\n",
      "loss: 0.959296  [21728/200000]\n",
      "loss: 0.486723  [21760/200000]\n",
      "loss: 2.264670  [21792/200000]\n",
      "loss: 1.365486  [21824/200000]\n",
      "loss: 1.708674  [21856/200000]\n",
      "loss: 0.676032  [21888/200000]\n",
      "loss: 0.295240  [21920/200000]\n",
      "loss: 1.075637  [21952/200000]\n",
      "loss: 0.386649  [21984/200000]\n",
      "loss: 0.999926  [22016/200000]\n",
      "loss: 1.725989  [22048/200000]\n",
      "loss: 0.180679  [22080/200000]\n",
      "loss: 2.046959  [22112/200000]\n",
      "loss: 0.357351  [22144/200000]\n",
      "loss: 1.011670  [22176/200000]\n",
      "loss: 1.157026  [22208/200000]\n",
      "loss: 2.102333  [22240/200000]\n",
      "loss: 0.291370  [22272/200000]\n",
      "loss: 1.555328  [22304/200000]\n",
      "loss: 1.460059  [22336/200000]\n",
      "loss: 0.727280  [22368/200000]\n",
      "loss: 2.257751  [22400/200000]\n",
      "loss: 1.532537  [22432/200000]\n",
      "loss: 1.458735  [22464/200000]\n",
      "loss: 1.609260  [22496/200000]\n",
      "loss: 2.294427  [22528/200000]\n",
      "loss: 1.438387  [22560/200000]\n",
      "loss: 0.270271  [22592/200000]\n",
      "loss: 0.071762  [22624/200000]\n",
      "loss: 2.422611  [22656/200000]\n",
      "loss: 1.274558  [22688/200000]\n",
      "loss: 1.338801  [22720/200000]\n",
      "loss: 2.354664  [22752/200000]\n",
      "loss: 1.621150  [22784/200000]\n",
      "loss: 1.930642  [22816/200000]\n",
      "loss: 0.447066  [22848/200000]\n",
      "loss: 1.132116  [22880/200000]\n",
      "loss: 1.878153  [22912/200000]\n",
      "loss: 1.804570  [22944/200000]\n",
      "loss: 2.129035  [22976/200000]\n",
      "loss: 1.801131  [23008/200000]\n",
      "loss: 0.262474  [23040/200000]\n",
      "loss: 0.178792  [23072/200000]\n",
      "loss: 1.535583  [23104/200000]\n",
      "loss: 0.988722  [23136/200000]\n",
      "loss: 1.159345  [23168/200000]\n",
      "loss: 1.644835  [23200/200000]\n",
      "loss: 1.593113  [23232/200000]\n",
      "loss: 1.545614  [23264/200000]\n",
      "loss: 1.426743  [23296/200000]\n",
      "loss: 0.899053  [23328/200000]\n",
      "loss: 1.699199  [23360/200000]\n",
      "loss: 1.433046  [23392/200000]\n",
      "loss: 1.709444  [23424/200000]\n",
      "loss: 2.011751  [23456/200000]\n",
      "loss: 2.373055  [23488/200000]\n",
      "loss: 1.140076  [23520/200000]\n",
      "loss: 0.895888  [23552/200000]\n",
      "loss: 0.372400  [23584/200000]\n",
      "loss: 0.642113  [23616/200000]\n",
      "loss: 1.603959  [23648/200000]\n",
      "loss: 0.666358  [23680/200000]\n",
      "loss: 0.446314  [23712/200000]\n",
      "loss: 1.335410  [23744/200000]\n",
      "loss: 0.740571  [23776/200000]\n",
      "loss: 1.394978  [23808/200000]\n",
      "loss: 2.016069  [23840/200000]\n",
      "loss: 1.378239  [23872/200000]\n",
      "loss: 2.772881  [23904/200000]\n",
      "loss: 1.232809  [23936/200000]\n",
      "loss: 1.497244  [23968/200000]\n",
      "loss: 0.390000  [24000/200000]\n",
      "loss: 1.200318  [24032/200000]\n",
      "loss: 0.747929  [24064/200000]\n",
      "loss: 1.149770  [24096/200000]\n",
      "loss: 1.366751  [24128/200000]\n",
      "loss: 1.749041  [24160/200000]\n",
      "loss: 0.371655  [24192/200000]\n",
      "loss: 0.873759  [24224/200000]\n",
      "loss: 1.476035  [24256/200000]\n",
      "loss: 1.431433  [24288/200000]\n",
      "loss: 0.831801  [24320/200000]\n",
      "loss: 1.365620  [24352/200000]\n",
      "loss: 1.294813  [24384/200000]\n",
      "loss: 0.338932  [24416/200000]\n",
      "loss: 2.448913  [24448/200000]\n",
      "loss: 2.360392  [24480/200000]\n",
      "loss: 0.072506  [24512/200000]\n",
      "loss: 0.196282  [24544/200000]\n",
      "loss: 0.177088  [24576/200000]\n",
      "loss: 1.957629  [24608/200000]\n",
      "loss: 1.159667  [24640/200000]\n",
      "loss: 0.445258  [24672/200000]\n",
      "loss: 0.444306  [24704/200000]\n",
      "loss: 0.441207  [24736/200000]\n",
      "loss: 0.398851  [24768/200000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 15\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Set the model to training mode - important for batch normalization and dropout layers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Unnecessary in this situation but added for best practices\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(X)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mNeutrinoDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     33\u001b[0m         pulse_array_sensors[pulse[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m pulse[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m meta_vals[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Add charge\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     pulse_array_sensors[pulse[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pulse[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     37\u001b[0m flattened_pulse \u001b[38;5;241m=\u001b[39m (pulse_array_sensors[:, \u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(flattened_pulse.shape)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcff095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# meta_vals = np.array(batch_104_vals_df.loc[batch_104_vals_df['event_id'] \n",
    "#                                            == df.index[0]])[0].astype(float)\n",
    "        \n",
    "# pulse_array = np.array(df.loc[df.index[i]])\n",
    "# pulse_array_sensors = np.concatenate((np.expand_dims(np.arange(5160), axis=1), np.zeros([5160, 3])), 1)\n",
    "\n",
    "# for pulse in pulse_array:\n",
    "#     if(pulse_array_sensors[pulse[0]][1] == 0):\n",
    "#         pulse_array_sensors[pulse[0]][1] = pulse[1] - meta_vals[2] # first time\n",
    "#     else:\n",
    "#         # possible last time, will be the last time for the actual last one\n",
    "#         pulse_array_sensors[pulse[0]][2] = pulse[1] - meta_vals[2]\n",
    "#     # Add charge\n",
    "#     pulse_array_sensors[pulse[0]][3] += pulse[2]\n",
    "\n",
    "# print(torch.from_numpy(np.concatenate(\n",
    "#     (np.ndarray.flatten(pulse_array_sensors[:, 1:]), meta_vals[-2:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3546a0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7386982244869325\n",
      "1.0650035905634065\n",
      "1.3417156555555945\n",
      "1.4730707044898226\n",
      "1.3924171977633228\n",
      "1.4721141445653219\n",
      "1.4830690410494232\n",
      "1.470363600995138\n",
      "1.5334018653103512\n",
      "1.5735355753053124\n",
      "1.5645000481634719\n",
      "1.6330845664236076\n",
      "1.65312866652555\n",
      "1.598865237073891\n",
      "1.5528595768506936\n",
      "1.5781901293591631\n",
      "1.5610519613869396\n",
      "1.5757805201572639\n",
      "1.602080908501265\n",
      "1.5766664047995897\n",
      "1.5750081957786746\n",
      "1.5545708645505676\n",
      "1.5747693584513602\n",
      "1.5934875309918348\n",
      "1.6113250947012483\n",
      "1.6240856069012108\n",
      "1.6056152847478444\n",
      "1.56305852801106\n",
      "1.5718101375047544\n",
      "1.615236531500848\n",
      "1.6282648729784444\n",
      "1.5986009119889812\n",
      "1.569299928977477\n",
      "1.5552178964019945\n",
      "1.5579168600255013\n",
      "1.5583911958286982\n",
      "1.542998378330522\n",
      "1.5415232607956573\n",
      "1.5630536148069154\n",
      "1.573990463576131\n",
      "1.5629801134730839\n",
      "1.5628833075891115\n",
      "1.5383684304846361\n",
      "1.5332331651338713\n",
      "1.528977149286577\n",
      "1.5351189543287431\n",
      "1.523000570331043\n",
      "1.5376928455547125\n",
      "1.5569109908416001\n",
      "1.538880775476145\n",
      "1.526095839755001\n",
      "1.5443503164309829\n",
      "1.5364453464749328\n",
      "1.5445650621859104\n",
      "1.5522812639386245\n",
      "1.554518719125419\n",
      "1.55952235292471\n",
      "1.5466814720033584\n",
      "1.5675591786772944\n",
      "1.5526710217056372\n",
      "1.5524935297983375\n",
      "1.5688390554904044\n",
      "1.5728154841831175\n",
      "1.5652842739330588\n",
      "1.5617650304142303\n",
      "1.5688669636870094\n",
      "1.5553784321320359\n",
      "1.5448008829908786\n",
      "1.5377590349319208\n",
      "1.5450203342117808\n",
      "1.5423010574642528\n",
      "1.5324122682270218\n",
      "1.523433265132023\n",
      "1.5122073870195485\n",
      "1.5126200641242102\n",
      "1.5068683807248693\n",
      "1.5191513682695075\n",
      "1.5243859111149431\n",
      "1.5274402540908742\n",
      "1.5330206164050644\n",
      "1.5176254159927036\n",
      "1.5129751874591504\n",
      "1.512623797913169\n",
      "1.5169726754962767\n",
      "1.51618813701467\n",
      "1.5027701761220569\n",
      "1.5083078924050524\n",
      "1.5096144763090045\n",
      "1.5048716891622445\n",
      "1.4958425670908058\n",
      "1.4869693896376395\n",
      "1.4866358207247976\n",
      "1.4776177537255217\n",
      "1.4833213972840131\n",
      "1.4786347322920783\n",
      "1.4786395462304422\n",
      "1.4856643143121608\n",
      "1.486619857259734\n",
      "1.489955567826082\n",
      "1.4976435913195232\n",
      "1.5006655875277342\n",
      "1.5024881572911561\n",
      "1.495098906473313\n",
      "1.4957392876561593\n",
      "1.499605305397826\n",
      "1.5019745629692898\n",
      "1.4995960257090082\n",
      "1.505426061464866\n",
      "1.5030691747988607\n",
      "1.5058271944319346\n",
      "1.4982935843436116\n",
      "1.503613330147282\n",
      "1.5045526659005797\n",
      "1.500507254912753\n",
      "1.5112172849180376\n",
      "1.5043258066438758\n",
      "1.5083988723566912\n",
      "1.510168787138227\n",
      "1.5134202000121313\n",
      "1.5187514675397396\n",
      "1.516672067056938\n",
      "1.5202907256629157\n",
      "1.5130598796692378\n",
      "1.5053311927933783\n",
      "1.5039693646172956\n",
      "1.5067232401803101\n",
      "1.510524758793569\n",
      "1.5129570340170282\n",
      "1.511003562615863\n",
      "1.5110993498463172\n",
      "1.5119595098451064\n",
      "1.5147558339048068\n",
      "1.5119237608455391\n",
      "1.5139846649302822\n",
      "1.5121746472854198\n",
      "1.5088254530850738\n",
      "1.5022548944915879\n",
      "1.4961766652012944\n",
      "1.4906187278014686\n",
      "1.493497578801093\n",
      "1.4938937551383462\n",
      "1.4956043613369643\n",
      "1.5019283198891547\n",
      "1.4983035344480753\n",
      "1.5005004425093176\n",
      "1.4997379082888662\n",
      "1.5034966077393312\n",
      "1.503658451179356\n",
      "1.501743709194303\n",
      "1.499175904923698\n",
      "1.5014104798315644\n",
      "1.4983970938674642\n",
      "1.4932394948844239\n",
      "1.4934616373648042\n",
      "1.4958818391778608\n",
      "1.4873640123805514\n",
      "1.4837313091541116\n",
      "1.4811243989830427\n",
      "1.4759403424129802\n",
      "1.4757652899221827\n",
      "1.4738410536269386\n",
      "1.4807565172186126\n",
      "1.4799494294205602\n",
      "1.4865930697517227\n",
      "1.489450980947083\n",
      "1.491554702276111\n",
      "1.4917874237315754\n",
      "1.4837296323917748\n",
      "1.4857007896162506\n",
      "1.4780055565202503\n",
      "1.47856069767409\n",
      "1.4806511763353056\n",
      "1.4812139673363574\n",
      "1.4787693122304884\n",
      "1.4750258667228666\n",
      "1.4713591467689129\n",
      "1.4776928191541547\n",
      "1.471358843351583\n",
      "1.4717152357525625\n",
      "1.4707048197973613\n",
      "1.4709630532770637\n",
      "1.4664545329809269\n",
      "1.4624777225519978\n",
      "1.460596545982099\n",
      "1.4541948958221662\n",
      "1.4529066063339697\n",
      "1.45104669800877\n",
      "1.4517670613054399\n",
      "1.451420097999789\n",
      "1.4517407929224322\n",
      "1.4513023694853737\n",
      "1.4496617669163943\n",
      "1.4486300136592054\n",
      "1.4470200995488038\n",
      "1.449667350016461\n",
      "1.450073066858072\n",
      "1.452042863612108\n",
      "1.4541679682672017\n",
      "1.4542182346986507\n",
      "1.4587379700990484\n",
      "1.4622472786198342\n",
      "1.4641611404192696\n",
      "1.4621648459685481\n",
      "1.4606027205466399\n",
      "1.4619619482910313\n",
      "1.4638435504759049\n",
      "1.4626897638959513\n",
      "1.4611420491066176\n",
      "1.459122312452818\n",
      "1.4607748491809343\n",
      "1.4586719679471225\n",
      "1.4604007049766958\n",
      "1.46212238388995\n",
      "1.4601031801759015\n",
      "1.459603017110708\n",
      "1.462187869237969\n",
      "1.4560954269463577\n",
      "1.4616787745070403\n",
      "1.4616219531214028\n",
      "1.4607784264003478\n",
      "1.4630986525965932\n",
      "1.4653286064902402\n",
      "1.4659330171300735\n",
      "1.4662611816150606\n",
      "1.464084956396605\n",
      "1.4658403953160715\n",
      "1.4682001992571354\n",
      "1.4706769027693893\n",
      "1.475829141045734\n",
      "1.4789710344226876\n",
      "1.4770095592311305\n",
      "1.4778173843745652\n",
      "1.4739196289544363\n",
      "1.4750531339819686\n",
      "1.4752887342005077\n",
      "1.4752626674097207\n",
      "1.4745392301415068\n",
      "1.4767536966062909\n",
      "1.4760004728192158\n",
      "1.4767621624500447\n",
      "1.4788286253338654\n",
      "1.4803319815381022\n",
      "1.4808320202263352\n",
      "1.482832528990101\n",
      "1.483218982563497\n",
      "1.486220981870562\n",
      "1.4878087933533888\n",
      "1.4879806094501036\n",
      "1.4901703076411155\n",
      "1.4892279397589459\n",
      "1.4923247179277324\n",
      "1.4924676017936631\n",
      "1.4937826081826868\n",
      "1.4908098350185304\n",
      "1.489055191796059\n",
      "1.4837050184043816\n",
      "1.4822277774818766\n",
      "1.4845988212961403\n",
      "1.486791434911909\n",
      "1.4864433102283123\n",
      "1.4866555734663662\n",
      "1.4833751683947831\n",
      "1.484987337693542\n",
      "1.487705910199719\n",
      "1.4825825090839917\n",
      "1.4804868742917188\n",
      "1.4810217072567415\n",
      "1.482842358194534\n",
      "1.4832752044551027\n",
      "1.481342505172067\n",
      "1.4797755004889168\n",
      "1.4810147641022964\n",
      "1.4835823590269082\n",
      "1.4848457110119988\n",
      "1.4853491348845107\n",
      "1.482576142125333\n",
      "1.4871965783461558\n",
      "1.488412289280942\n",
      "1.4896166139798859\n",
      "1.4874091425970088\n",
      "1.4876075377118565\n",
      "1.489570331837569\n",
      "1.4909972378692966\n",
      "1.4921420227132793\n",
      "1.4932815449470898\n",
      "1.4944673880758084\n",
      "1.4945733325656114\n",
      "1.491597192243488\n",
      "1.4875833570233583\n",
      "1.488042125485523\n",
      "1.4865707897639164\n",
      "1.4852806882440899\n",
      "1.481945833777282\n",
      "1.4830821860525445\n",
      "1.4792197745751923\n",
      "1.4788950269734473\n",
      "1.4803697307790327\n",
      "1.4786939363149343\n",
      "1.477397965224567\n",
      "1.4788576860846034\n",
      "1.479273960705404\n",
      "1.480462202648112\n",
      "1.481843535564933\n",
      "1.482158440357654\n",
      "1.483792198477932\n",
      "1.4838717749045431\n",
      "1.4829790614033158\n",
      "1.4847216946681254\n",
      "1.4865558510419203\n",
      "1.4853907899377863\n",
      "1.4811419185634773\n",
      "1.48211628934246\n",
      "1.4868582998483268\n",
      "1.4867309789187484\n",
      "1.485993986405034\n",
      "1.487918706754383\n",
      "1.4896714077172697\n",
      "1.49081739625873\n",
      "1.488923230693685\n",
      "1.4875234017621026\n",
      "1.4861812236757357\n",
      "1.4825622006838353\n",
      "1.481657893959553\n",
      "1.4827088473296024\n",
      "1.4828176428944588\n",
      "1.4853955431243238\n",
      "1.483087637270697\n",
      "1.4816620593627245\n",
      "1.4783475493407066\n",
      "1.4744438029141225\n",
      "1.4733110943489673\n",
      "1.4727270313199052\n",
      "1.4729856185062493\n",
      "1.4697315483283593\n",
      "1.469878398139304\n",
      "1.4676788169032728\n",
      "1.466424490973752\n",
      "1.466939596876727\n",
      "1.4663910092813222\n",
      "1.4674767125118682\n",
      "1.4685234235731983\n",
      "1.4683097363275166\n",
      "1.4656160404506289\n",
      "1.4679216854410926\n",
      "1.464716821136191\n",
      "1.4636516007944165\n",
      "1.4638672942792161\n",
      "1.4655138241247454\n",
      "1.46577333778504\n",
      "1.4667311916445582\n",
      "1.467728695951349\n",
      "1.466169895923187\n",
      "1.464963995954059\n",
      "1.464115918813702\n",
      "1.4665877051053033\n",
      "1.4644337314200042\n",
      "1.4639471601131162\n",
      "1.4629146626052116\n",
      "1.4638707638942146\n",
      "1.460141550237043\n",
      "1.4630439077639377\n",
      "1.4643786615396395\n",
      "1.4653115079334569\n",
      "1.4634477913339512\n",
      "1.4602389243588716\n",
      "1.4604249663815878\n",
      "1.4582420348163945\n",
      "1.461327278763722\n",
      "1.463577808001299\n",
      "1.4650012800627346\n",
      "1.4643009092239567\n",
      "1.4674492346023098\n",
      "1.4665448993894534\n",
      "1.4683536354707145\n",
      "1.4692162129786126\n",
      "1.4693706822491468\n",
      "1.467615959507618\n",
      "1.4691785426359278\n",
      "1.471057331962301\n",
      "1.4722044497082394\n",
      "1.4751587511380662\n",
      "1.4771832755460432\n",
      "1.4786497179933729\n",
      "1.477436345460709\n",
      "1.4778212887536244\n",
      "1.478149034739993\n",
      "1.4790722375990362\n",
      "1.4783914656309425\n",
      "1.4791817731390464\n",
      "1.4771925977549576\n",
      "1.4768328556768355\n",
      "1.4747017282905097\n",
      "1.4718146280635898\n",
      "1.4720569979741032\n",
      "1.4714990553706868\n",
      "1.4723921492162575\n",
      "1.4737457691921614\n",
      "1.4746362736270444\n",
      "1.4754939651801802\n",
      "1.4735358570240615\n",
      "1.476054149134139\n",
      "1.474831031806034\n",
      "1.4760093050821643\n",
      "1.4758002207647771\n",
      "1.476991203965324\n",
      "1.477908890651062\n",
      "1.4788819330545648\n",
      "1.4789911829798599\n",
      "1.479317172746282\n",
      "1.4793578282247475\n",
      "1.4772512763665027\n",
      "1.4768840636921154\n",
      "1.4788377430081407\n",
      "1.476800768026483\n",
      "1.476639168999526\n",
      "1.4795039265642003\n",
      "1.478657081298741\n",
      "1.4784075002678003\n",
      "1.4790890423759442\n",
      "1.480189131476566\n",
      "1.478967423574698\n",
      "1.4820401213672034\n",
      "1.481118185914414\n",
      "1.480758826124563\n",
      "1.483002780870772\n",
      "1.4841953767755287\n",
      "1.4849744396011788\n",
      "1.483454818284452\n",
      "1.4842834922921517\n",
      "1.4859802033453597\n",
      "1.4867949279961858\n",
      "1.4842328207395\n",
      "1.4839338354252285\n",
      "1.4827391659142708\n",
      "1.4824194175256895\n",
      "1.4827425089441004\n",
      "1.482877752665877\n",
      "1.482468827184625\n",
      "1.481227803455815\n",
      "1.4814174763962058\n",
      "1.4799064104179502\n",
      "1.4792299989585116\n",
      "1.4802466633085607\n",
      "1.4803114176473215\n",
      "1.4782775124411656\n",
      "1.4775168107246823\n",
      "1.4762475768058947\n",
      "1.4763579081573002\n",
      "1.475889766174459\n",
      "1.4747848824077925\n",
      "1.4759560381223449\n",
      "1.4744211907930098\n",
      "1.473593784681068\n",
      "1.4726461680039915\n",
      "1.4718461827430274\n",
      "1.4743853062345642\n",
      "1.4725547896190787\n",
      "1.471768360048071\n",
      "1.4720065278454664\n",
      "1.4722268970818313\n",
      "1.4713583777869148\n",
      "1.4703873350540082\n",
      "1.4714727049862522\n",
      "1.4703452383921622\n",
      "1.4700157502282527\n",
      "1.4707778714041935\n",
      "1.471552143961278\n",
      "1.472186475717234\n",
      "1.4729667746728903\n",
      "1.4731691036240104\n",
      "1.472453896771818\n",
      "1.4726031067743668\n",
      "1.4736975240343204\n",
      "1.4739053881488189\n",
      "1.4731031253255398\n",
      "1.4740897954670922\n",
      "1.4729624926113472\n",
      "1.4736819334287463\n",
      "1.4744349149748945\n",
      "1.4741613986825963\n",
      "1.473699479661501\n",
      "1.4731881288416826\n",
      "1.4716701830814325\n",
      "1.470022661546921\n",
      "1.4690564533462918\n",
      "1.4696969237293749\n",
      "1.4706999042083417\n",
      "1.4714762814504867\n",
      "1.4734455315768935\n",
      "1.4718646620530162\n",
      "1.4717170992361035\n",
      "1.4719908335829583\n",
      "1.472591129196685\n",
      "1.4719103914376659\n",
      "1.4716174246605473\n",
      "1.4713291986695385\n",
      "1.4709716154410055\n",
      "1.4720714118604685\n",
      "1.4731325558221913\n",
      "1.4747161686484906\n",
      "1.4755478057052218\n",
      "1.4751865448136114\n",
      "1.476637557721458\n",
      "1.4761118075400912\n",
      "1.476107097826812\n",
      "1.476512418166733\n",
      "1.4756906647393786\n",
      "1.4763786048052787\n",
      "1.4773463406673217\n",
      "1.4782352597352382\n",
      "1.4785191188667794\n",
      "1.4803217185424147\n",
      "1.479651123918046\n",
      "1.4785856441776735\n",
      "1.4792586122826035\n",
      "1.4784687106237395\n",
      "1.480211096741972\n",
      "1.4811957519041157\n",
      "1.4812540887658951\n",
      "1.4816839064270364\n",
      "1.48013416718318\n",
      "1.4802523852831366\n",
      "1.4804478733200197\n",
      "1.4786999725555636\n",
      "1.4787952850556263\n",
      "1.4790699846580926\n",
      "1.4789575149883956\n",
      "1.4796654255337645\n",
      "1.479916192210348\n",
      "1.4801150608103857\n",
      "1.4802986004181855\n",
      "1.4798060438663676\n",
      "1.4796211557193055\n",
      "1.4791050799412842\n",
      "1.4785582476837\n",
      "1.4782650685817615\n",
      "1.4779124193054896\n",
      "1.4768664503184419\n",
      "1.4778725166894073\n",
      "1.4777663588494396\n",
      "1.478385433755754\n",
      "1.4784814369874566\n",
      "1.4785967252095076\n",
      "1.4791561339655193\n",
      "1.4802095399231456\n",
      "1.4802264429553296\n",
      "1.4819001932941023\n",
      "1.4832569255285155\n",
      "1.483055710000696\n",
      "1.4837221032522858\n",
      "1.4823699728008062\n",
      "1.481574693526327\n",
      "1.4835773388069242\n",
      "1.482899176758329\n",
      "1.4816798075763657\n",
      "1.48131432154373\n",
      "1.4822349268213812\n",
      "1.4828772244793267\n",
      "1.4818579359458355\n",
      "1.4827643366890293\n",
      "1.483465212933239\n",
      "1.4840997491981474\n",
      "1.4841548204731143\n",
      "1.4848768966649328\n",
      "1.4841190335487795\n",
      "1.4857349439374858\n",
      "1.4852383676021594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;66;03m# print(X)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(model(X))\n\u001b[0;32m      8\u001b[0m         y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [2], line 23\u001b[0m, in \u001b[0;36mNeutrinoDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\n\u001b[0;32m     21\u001b[0m sg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_geom\n\u001b[0;32m     22\u001b[0m meta_vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvals_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvals_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     25\u001b[0m pulse_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mindex[i]])\n\u001b[0;32m     26\u001b[0m pulse_array_sensors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mexpand_dims(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m5160\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m5160\u001b[39m, \u001b[38;5;241m3\u001b[39m])), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1375\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:1171\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getbool_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, axis: AxisInt):\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# caller is responsible for ensuring non-None axis\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m-> 1171\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m     inds \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:2594\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_like(result):\n\u001b[0;32m   2591\u001b[0m     \u001b[38;5;66;03m# GH 33924\u001b[39;00m\n\u001b[0;32m   2592\u001b[0m     \u001b[38;5;66;03m# key may contain nan elements, check_array_indexer needs bool array\u001b[39;00m\n\u001b[0;32m   2593\u001b[0m     result \u001b[38;5;241m=\u001b[39m pd_array(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m-> 2594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_array_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:533\u001b[0m, in \u001b[0;36mcheck_array_indexer\u001b[1;34m(array, indexer)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bool_dtype(dtype):\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m--> 533\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(indexer, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:637\u001b[0m, in \u001b[0;36mIndexOpsMixin.to_numpy\u001b[1;34m(self, dtype, copy, na_value, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03mA NumPy ndarray representing the values in this Series or Index.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype):\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mto_numpy(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, na_value\u001b[38;5;241m=\u001b[39mna_value, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kwargs:\n\u001b[0;32m    639\u001b[0m     bad_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys()))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:481\u001b[0m, in \u001b[0;36mBaseMaskedArray.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hasna\u001b[49m:\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    483\u001b[0m         dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_string_dtype(dtype)\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m libmissing\u001b[38;5;241m.\u001b[39mNA\n\u001b[0;32m    486\u001b[0m     ):\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    488\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-dtype NumPy array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith missing values. Specify an appropriate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:671\u001b[0m, in \u001b[0;36mBaseMaskedArray._hasna\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hasna\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;66;03m# Note: this is expensive right now! The hope is that we can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \n\u001b[0;32m    670\u001b[0m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"bool_\", expected \"bool\")\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:58\u001b[0m, in \u001b[0;36m_any\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_total = 0\n",
    "num = 0\n",
    "with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(X)\n",
    "        # Compute prediction and loss\n",
    "        pred = np.array(model(X))\n",
    "        y = np.array(y)\n",
    "        loss_total += angular_dist_score(y[0], y[1], pred[0], pred[1])\n",
    "        num +=1\n",
    "        print(loss_total/num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89053d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
