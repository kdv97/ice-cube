{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163fbc16",
   "metadata": {},
   "source": [
    "# Sensor-Based Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec4a5f",
   "metadata": {},
   "source": [
    "In this notebook, we perform several linear regressions which we call \"sensor-based\". This means that the features included make essential use of all 5,160 IceCube sensors. In essence, each data point in `X_train` fed into the `LinearRegression` object will be a 5160-tuple where the $i$th entry provides information about the (in)activation of the $i$th sensor for that event.\n",
    "\n",
    "Since the raw data is not conducive to directly being fed into `sklearn`'s `LinearRegression`, we first define various functions which help us extract these features and return the processed data that we feed into the regressor. So that our results on this notebook are consistent with results on others, we train and test using the data in batch 10. As batch 10 (and all other batches) consists of 200,000 events (each with up to thousands of pulses), we ran the functions below on the raw batch 10 data first and output the result as `.parquet` files. The processed data is then called directly by the `pandas.read_parquet` function to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2c0e8",
   "metadata": {},
   "source": [
    "# Importing Modules and Defining Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982bccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as LinearRegression\n",
    "from mae import angular_dist_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63225b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_event_to_proc_binary(event, aux_incl=False):\n",
    "    \"\"\"\n",
    "    Given an event, this function returns a processed\n",
    "    5160-tuple with the ith entry being 0 if that sensor_id\n",
    "    was not pinged during this event, and a 1 otherwise\n",
    "    \"\"\"\n",
    "    if aux_incl == False:\n",
    "        event = event[event.auxiliary==False]\n",
    "    \n",
    "    # array to be returned\n",
    "    proc = np.zeros((5160,))\n",
    "    \n",
    "    # find the sensors that got pinged, modify proc accordingly\n",
    "    sensors = np.unique(event.sensor_id.values)\n",
    "    for sensor in sensors:\n",
    "        proc[sensor] = 1\n",
    "    \n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e52fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_batch_to_proc_binary(batch, aux_incl=False):\n",
    "    \"\"\"\n",
    "    Given a (sub)batch, this function returns a processed\n",
    "    pandas DataFrame whose rows are the processed events\n",
    "    according to raw_event_to_proc_binary\n",
    "    \"\"\"\n",
    "    # DataFrame to be returned\n",
    "    event_ids = np.unique(batch.index)\n",
    "    df = pd.DataFrame(0, index=event_ids, columns=[i for i in range(5160)])\n",
    "\n",
    "    # run the raw_event_to_proc_binary function on each event\n",
    "    count = 0\n",
    "    for event_id in event_ids:\n",
    "        df.loc[event_id] = raw_event_to_proc_binary(batch.loc[event_id], aux_incl=aux_incl)\n",
    "        if count % 1000 == 0:\n",
    "            print('Working on',count)\n",
    "        count += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a26deffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_event_to_proc_chargesum(event, aux_incl=False):\n",
    "    \"\"\"\n",
    "    Given an event, this function returns a processed\n",
    "    5160-tuple with the ith entry being the sum of all\n",
    "    charges across all pulses registered by that sensor\n",
    "    in this event\n",
    "    \"\"\"\n",
    "    if aux_incl == False:\n",
    "        event = event[event.auxiliary==False]\n",
    "    \n",
    "    # array to be returned\n",
    "    proc = np.zeros((5160,))\n",
    "    \n",
    "    # find the sensors that got pinged, modify proc accordingly\n",
    "    event = event.drop(['time','auxiliary'], axis=1).groupby('sensor_id').sum()\n",
    "    for sensor in event.index:\n",
    "        proc[sensor] = event.loc[sensor].values[0]\n",
    "    \n",
    "    return proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd4b65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_batch_to_proc_chargesum(batch, aux_incl=False):\n",
    "    \"\"\"\n",
    "    Given a (sub)batch, this function returns a processed\n",
    "    pandas DataFrame whose rows are the processed events\n",
    "    according to raw_event_to_proc_chargesum\n",
    "    \"\"\"\n",
    "    # DataFrame to be returned\n",
    "    event_ids = np.unique(batch.index)\n",
    "    df = pd.DataFrame(0, index=event_ids, columns=[i for i in range(5160)])\n",
    "\n",
    "    # run the raw_event_to_proc_binary function on each event\n",
    "    count = 0\n",
    "    for event_id in event_ids:\n",
    "        df.loc[event_id] = raw_event_to_proc_chargesum(batch.loc[event_id], aux_incl=aux_incl)\n",
    "        if count % 1000 == 0:\n",
    "            print('Working on',count)\n",
    "        count += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ad69162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batch of our data\n",
    "batch10 = pd.read_parquet('../batches_train/batch_10.parquet')\n",
    "sensor_geom = pd.read_csv('../sensor_geometry.csv')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]\n",
    "\n",
    "# list of unique event ids\n",
    "event_ids = np.sort(np.unique(batch10.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850fb2f",
   "metadata": {},
   "source": [
    "# Model 0: Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49a5a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "az_avg = np.mean(meta10['azimuth'].values)\n",
    "ze_avg = np.mean(meta10['zenith'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split batch10, k-fold cross validation\n",
    "# this cell imitates the erdos lectures notes on kfold cross validation , k = 5\n",
    "# random seed to all splits random_seed = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc3ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "y_train, y_test = train_test_split(batch10_true_directions,\n",
    "                                   shuffle=True,\n",
    "                                   test_size=.25,\n",
    "                                   random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13b4ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "# cross validation on model 0\n",
    "maes_0 = []\n",
    "for train_index, test_index in kfold.split(y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # \"fit\" our model \n",
    "    az_avg = np.mean(y_tt['azimuth'])\n",
    "    ze_avg = np.mean(y_tt['zenith'])\n",
    "    \n",
    "    # predict \n",
    "    az_pred = az_avg*np.ones((len(y_ho),))\n",
    "    ze_pred = ze_avg*np.ones((len(y_ho),))\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_0.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22bc4390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV mae for model_0 (baseline) is: 1.5701685872315714\n"
     ]
    }
   ],
   "source": [
    "# This code was run locally and I'm saving the result here for \n",
    "# future use without needing to run it\n",
    "maes_0 = [1.5653397534634008, 1.5664915922174312, 1.5770938834588555, 1.570667153939643, 1.5712505530785261]\n",
    "avg_mae_0 = np.mean(maes_0)\n",
    "print(\"Average CV mae for model_0 (baseline) is:\", avg_mae_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503e387",
   "metadata": {},
   "source": [
    "# Model 1: Sensor binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb79f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed binary data from batch10 using \n",
    "# batch10_proc_binary = raw_batch_to_proc_binary(batch10)\n",
    "# but for convenience I've already run this and stored the \n",
    "# result as a .parquet file\n",
    "batch10_proc_binary = pd.read_parquet('../batches_train/batch10_proc_binary.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42048a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the targets are the azimuth (az) and zenith (ze)\n",
    "# which we extract from the provided meta data\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd937c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used to downsize the data for debugging purposes\n",
    "# so it runs faster. Comment out to run on full dataset. \n",
    "batch10_proc_binary = batch10_proc_binary[0:1000]\n",
    "batch10_true_directions = batch10_true_directions[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4227537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train test split on the whole batch10\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_proc_binary, \n",
    "                                                    batch10_true_directions,\n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3000d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split batch10, k-fold cross validation\n",
    "# this cell imitates the erdos lectures notes on kfold cross validation , k = 5\n",
    "# random seed to all splits random_seed = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7167b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on our training test we now perform k-fold cross validation\n",
    "# we use k = 5 and random seed 134\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, \n",
    "              shuffle=True,\n",
    "              random_state=134)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3b29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CROSS-VALIDATION ###\n",
    "\n",
    "# Defining model 1\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_1 = LinearRegression(copy_X=True)\n",
    "\n",
    "# cross validation on model 1\n",
    "maes_1 = []\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_1.fit(X_tt, y_tt)\n",
    "    \n",
    "    # predict \n",
    "    pred = model_1.predict(X_ho)\n",
    "    az_pred = pred[:,0]\n",
    "    ze_pred = pred[:,1]\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_1.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a962cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mae of model_1: 1.5644848014376325\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 30 mins)\n",
    "maes_1 = [1.5615248170191087, 1.5610988936989836, 1.5701054461389465, 1.5631204382457082, 1.5665744120854157]\n",
    "avg_mae_1 = np.mean(maes_1)\n",
    "print(\"Average CV mae of model_1:\", avg_mae_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5f607",
   "metadata": {},
   "source": [
    "# Model 2: sensor chargesum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3734cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "batch10_proc_chargesum = pd.read_parquet('../batches_train/batch10_proc_chargesum.parquet')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78666ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_proc_chargesum, \n",
    "                                                    batch10_true_directions, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on model_2\n",
    "\n",
    "# Defining model 2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_2 = LinearRegression(copy_X=True)\n",
    "\n",
    "# cross validation on model 2\n",
    "maes_2 = []\n",
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_2.fit(X_tt, y_tt)\n",
    "    \n",
    "    # predict \n",
    "    pred = model_2.predict(X_ho)\n",
    "    az_pred = pred[:,0]\n",
    "    ze_pred = pred[:,1]\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_2.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b0d4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mae of model_2: 1.5693874320109273\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 30 mins)\n",
    "maes_2 = [1.5660103902013034, 1.565257340669138, 1.576880378243591, 1.568163212969159, 1.5706258379714457]\n",
    "avg_mae_2 = np.mean(maes_2)\n",
    "print(\"Average mae of model_2:\", avg_mae_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ad9c2",
   "metadata": {},
   "source": [
    "# Model 3: sensor binary and num-clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946cc62",
   "metadata": {},
   "source": [
    "The processed data in this model combines the binary sensor activation of model 2 while also performing cluster analysis. We add classifier variables to the data to distinguish if data was clustered in 1, 2, 3, 4, or 5 cluster(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fe0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch10_binary_num_clusters = pd.read_parquet('../batches_train/batch10_binary_num_cluster.parquet')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b0182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_binary_num_clusters, \n",
    "                                                    batch10_true_directions, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c69f48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on model_3\n",
    "\n",
    "# Defining model 3\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_3 = LinearRegression(copy_X=True)\n",
    "\n",
    "# cross validation on model 3\n",
    "maes_3 = []\n",
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_3.fit(X_tt, y_tt)\n",
    "    \n",
    "    # predict \n",
    "    pred = model_3.predict(X_ho)\n",
    "    az_pred = pred[:,0]\n",
    "    ze_pred = pred[:,1]\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_3.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "418079d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average mae of model_3: 1.564471526731983\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 30 mins)\n",
    "maes_3 = [1.5614726430586565, 1.5612771545643698, 1.570034175849675, 1.5630600262171235, 1.5665136339700907]\n",
    "avg_mae_3 = np.mean(maes_3)\n",
    "print(\"Average CV mae of model_3:\", avg_mae_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce92bd4",
   "metadata": {},
   "source": [
    "# Model 4: Sensor chargesum and num-clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0388498",
   "metadata": {},
   "source": [
    "The processed data in this model combines the sensor chargesum of model 2 while also performing cluster analysis. We add classifier variables to the data to distinguish if data was clustered in 1, 2, 3, 4, or 5 cluster(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch10_binary_num_clusters = pd.read_parquet('../batches_train/batch10_chargesum_num_cluster.parquet')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb70944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_chargesum_num_clusters, \n",
    "                                                    batch10_true_directions, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on model_4\n",
    "\n",
    "# Defining model 4\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_4 = LinearRegression(copy_X=True)\n",
    "\n",
    "# cross validation on model 4\n",
    "maes_4 = []\n",
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_4.fit(X_tt, y_tt)\n",
    "    \n",
    "    # predict \n",
    "    pred = model_4.predict(X_ho)\n",
    "    az_pred = pred[:,0]\n",
    "    ze_pred = pred[:,1]\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_4.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55c6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV mae of model_4: 1.569280087068745\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 30 mins)\n",
    "maes_4 = [1.5659610694146888, 1.5652911088990729, 1.5768272185107415, 1.5677951311907221, 1.5705259073284994]\n",
    "avg_mae_4 = np.mean(maes_4)\n",
    "print(\"Average CV mae of model_4:\", avg_mae_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4cd09",
   "metadata": {},
   "source": [
    "# Model 5: Binary sensor data with MAE (mean absolute error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3e7a1",
   "metadata": {},
   "source": [
    "This model is similar to model 1 in that it uses the binary sensor activation data. However, the error function used in this regression is mean absolute error (using `sklearn.linear_model.SGDRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f0e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "model_5_az = SGDRegressor(loss='epsilon_insensitive',\n",
    "                       max_iter=50000)\n",
    "model_5_ze = SGDRegressor(loss='epsilon_insensitive',\n",
    "                       max_iter=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch10_proc_binary = pd.read_parquet('../batches_train/batch10_proc_binary.parquet')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_proc_binary, \n",
    "                                                    batch10_true_directions, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022df2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on model_5\n",
    "maes_5 = []\n",
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_5_az.fit(X_tt, y_tt['azimuth'])\n",
    "    model_5_ze.fit(X_tt, y_tt['zenith'])\n",
    "    \n",
    "    # predict \n",
    "    az_pred = model_5_az.predict(X_ho)\n",
    "    ze_pred = model_5_ze.predict(X_ho)\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_5.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a39789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV mae of model_5: 1.5649132038242584\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 5 mins)\n",
    "maes_5 = [1.5623128167452556, 1.561854543414195, 1.569177896837212, 1.5639737578049215, 1.5672470043197075]\n",
    "avg_mae_5 = np.mean(maes_5)\n",
    "print(\"Average CV mae of model_5:\", avg_mae_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eea7ef",
   "metadata": {},
   "source": [
    "# Model 6: Adding in Katja's event features\n",
    "\n",
    "For this model we append the time-based best-fit-line predictions `az_t_pred`, `ze_t_pred` to the `batch10_proc_binary` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a051e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch10_proc_model_6 = pd.read_parquet('../batches_train/batch10_proc_model_6.parquet')\n",
    "meta10 = pd.read_parquet('../batches_train/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b251a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_proc_model_6, \n",
    "                                                    batch10_true_directions, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on model_6\n",
    "\n",
    "# Defining model 6\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_6 = LinearRegression(copy_X=True)\n",
    "\n",
    "# cross validation on model 6\n",
    "maes_6 = []\n",
    "kfold = KFold(n_splits=5,\n",
    "              shuffle=True,\n",
    "              random_state=134)\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    # assign X_tt, y_tt and X_ho, y_ho\n",
    "    X_tt = X_train.iloc[train_index]\n",
    "    y_tt = y_train.iloc[train_index]\n",
    "    X_ho = X_train.iloc[test_index]\n",
    "    y_ho = y_train.iloc[test_index]\n",
    "    \n",
    "    # fit our model \n",
    "    model_6.fit(X_tt, y_tt)\n",
    "    \n",
    "    # predict \n",
    "    pred = model_6.predict(X_ho)\n",
    "    az_pred = pred[:,0]\n",
    "    ze_pred = pred[:,1]\n",
    "    \n",
    "    # get error according to custom error function\n",
    "    err = angular_dist_score(y_ho['azimuth'].values, \n",
    "                             y_ho['zenith'].values,\n",
    "                             az_pred,\n",
    "                             ze_pred)\n",
    "    maes_6.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fd5a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV mae of model_6: 1.5107591842074226\n"
     ]
    }
   ],
   "source": [
    "# This code was run on the Great Lakes Cluster to save compute time\n",
    "# and therefore we define maes here ourselves to be the output of that\n",
    "# job (the job could be run locally from this notebook and output the\n",
    "# same result, it would just take > 30 mins)\n",
    "maes_6 = [1.5085980143977527, 1.5081281425142634, 1.5139578294166005, 1.5093864031516566, 1.5137255315568392]\n",
    "avg_mae_6 = np.mean(maes_6)\n",
    "print(\"Average CV mae of model_6:\", avg_mae_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116446b",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "From the k-fold cross-validation performed above, we believe model 6 has the lowest generalization error. Below we train this model on the full training set `X_train, y_train` and test on the full test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51816f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "batch10_proc_model_6 = pd.read_parquet('../data/batch10_proc_model_6.parquet')\n",
    "meta10 = pd.read_parquet('../data/batch10_meta.parquet')\n",
    "batch10_true_directions = meta10[['azimuth', 'zenith']]\n",
    "\n",
    "# Create train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(batch10_proc_model_6,\n",
    "                                                    batch10_true_directions,\n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=134)\n",
    "\n",
    "# Create and train the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_6 = LinearRegression(copy_X=True)\n",
    "model_6.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = model_6.predict(X_test)\n",
    "az_pred = pred[:,0]\n",
    "ze_pred = pred[:,1]\n",
    "\n",
    "# get error according to custom error function\n",
    "err = angular_dist_score(y_test['azimuth'].values,\n",
    "                         y_test['zenith'].values,\n",
    "                         az_pred,\n",
    "                         ze_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a208e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of model 6 on full test set: 1.5120286550902189\n"
     ]
    }
   ],
   "source": [
    "# We ran the code above on the Great Lakes Cluster to save compute time\n",
    "# and we record the result here\n",
    "err = 1.5120286550902189\n",
    "print(\"Error of model 6 on full test set:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaece2e",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba91b1c",
   "metadata": {},
   "source": [
    "We obtained a model (model 6) that performs better on average (as verified by 5-fold cross-validation) than the baseline model 0 which simply guesses the mean value of the training set's target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a065b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
