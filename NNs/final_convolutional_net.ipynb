{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce67c39d-d913-4cfc-b690-71a91c8422d0",
   "metadata": {},
   "source": [
    "FINAL CONVOLUTION NEURAL NETWORK\n",
    "The neural net is trained on a combination of provided and original features. The file sensor_geometry.csv contains the (x,y,z) positions of the sensors in the IceCube detector. The file train_meta.parquet provides additional information about each neutrino event. Most importantly, it contains the true azimuth and zenith of the incoming neutrino, which we train against. The neutrino events are stored in files of the form batch_##.parquet, where ## ranges from 1 to 660. Each batch file contains the information on ~200,000 neutrino events. A single neutrino event consists of an arbitrary number of rows in the parquet file, with each row corresponding to a single sensor activation during the neutrino event. \n",
    "\n",
    "The all-features.csv file consist of original features derived from the linear regression and clustering computations. We utilize just an initial guess of the azimuth and zenith and an estimate of the number of sensor activation clusters in the event. Additionally we compute a metric of potential scattering in a sensor.\n",
    "\n",
    "In this network, we encode the position of the sensors by their indices in an appropriately sized three-dimensional tensor with four channels. The four channels are the first time the sensor was activated, the last time it was activated, the total charge detected in this sensor and the above mentioned scatter metric. We used two layers of convolution followed by Max-Pool layers, then flattened the tensor to an array and added in the pre-comptued features. Finally we run three layers of linear layers with ReLu-activation to arrive at our predictions of azimuth and zenith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab16641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Take Care of the Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd.profiler as profiler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8293faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set parent directories for:\n",
    "\n",
    "sensor_geometry.csv\n",
    "batch_##.parquet\n",
    "train_meta.parquet\n",
    "all-features.csv\n",
    "\n",
    "respectively. \n",
    "\n",
    "'''\n",
    "pre_dir = \"D:/jupyter/erdos-data/\"\n",
    "sensor_geom_dir = \"D:/jupyter/erdos-data/\"\n",
    "batch_dir = \"D:/jupyter/erdos-data/train/\"\n",
    "meta_dir = \"D:/jupyter/erdos-data/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e4a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paths using specified directories. \n",
    "pre_path=pre_dir+\"all-features.csv\"\n",
    "sensor_geom_path=sensor_geom_dir+\"sensor_geometry.csv\"\n",
    "meta_path=meta_dir+\"train_meta.parquet\"\n",
    "\n",
    "#Load precompiled features and sensor geometry\n",
    "pre_feature=pd.read_csv(pre_path)\n",
    "sensor_geom = pd.read_csv(sensor_geom_path)\n",
    "\n",
    "\n",
    "#Load metadata, if it is not already loaded. \n",
    "try: meta\n",
    "except NameError: meta=pd.read_parquet(meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea740321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "#Set Default Device to CUDA or CPU\n",
    "#To be called in .to(device) to ensure all pytorch Tensors are on the same device\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc3ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Torch Dataset Wrapper.\n",
    "Initialization takes the filename, the sensor geometry file name, the batch ID, and the auxiliary flag.\n",
    "'''\n",
    "\n",
    "class NeutrinoDataset(Dataset):\n",
    "    def __init__(self, batch_filename, sensor_file_name, batch_id, aux):\n",
    "        \n",
    "        #save sensor_geometry and meta_data for the given patch\n",
    "        self.sensor_geom = pd.read_csv(sensor_file_name)\n",
    "        self.vals_df = meta[meta.batch_id==batch_id]\n",
    "        \n",
    "        #Loads the parquet file as a panda dataframe and filter by aux flag\n",
    "        self.dataframe = pd.read_parquet(batch_filename)\n",
    "        self.dataframe = self.dataframe[self.dataframe.auxiliary==aux]\n",
    "        \n",
    "        #set the number of features per sensor. \n",
    "        #Currently, first and last time, cumulative charge, scattering flag, x,y,z.  \n",
    "        self.num_features = 5160*(3+1+3) \n",
    "        \n",
    "        #Number of neutrino events in the data frame\n",
    "        self.num_events = self.dataframe.index.nunique()\n",
    "        \n",
    "        #Since an event can span multiple rows, save the unique indices, to obtain event_ids.  \n",
    "        self.unique_indices = np.unique(self.dataframe.index)\n",
    "        \n",
    "        #Generate the xyz-lists for the sensor locations. \n",
    "        self.x_values, self.y_values, self.z_values=self.get_sensor_pos()\n",
    "    \n",
    "    #Return number of Neutrino Events in the Dataset (not the number of rows)\n",
    "    def __len__(self):\n",
    "        return self.num_events\n",
    "\n",
    "    #Get the ith Neutrino Event, based on the ith unique event id\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        df = self.dataframe \n",
    "        sg = self.sensor_geom\n",
    "        #Get the event id corresponding to the ith unique index\n",
    "        event_id=self.unique_indices[i]\n",
    "        \n",
    "        #Load the ith neutrino event, and convert to np.array\n",
    "        event=df.loc[event_id]\n",
    "        pulse_array = np.array(event)\n",
    "        \n",
    "        #Load the metavalues associated to the event\n",
    "        meta_vals = np.array(\n",
    "            self.vals_df.loc[self.vals_df['event_id'] == event_id])[0].astype(float)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We will fill the following tensor with the data for all sensors\n",
    "        # There are 4 channels (1st time, last time, charge, and scattering)\n",
    "        # There are 118 x-coordinate values amongst the sensors\n",
    "        # There are 117 y-coordinate values amongst the sensors\n",
    "        # There are 4974 z-coordinate values amongst the sensors, however for memory-saving purposes we block them into \n",
    "        # 207-blocks\n",
    "        \n",
    "        formated_pulse=torch.zeros(4,22,20,207).to(device)\n",
    "        \n",
    "        #Find pulse with largest charge\n",
    "        loudest=self.loudest_bang(event)\n",
    "        \n",
    "        \n",
    "        for pulse in pulse_array:\n",
    "            #Gives sensor_id\n",
    "            sensor_id=pulse[0] \n",
    "            \n",
    "            #Computes (x,y,z) of this sensor_id\n",
    "            sensor_xyz=self.id_to_xyz(sensor_id)\n",
    "            \n",
    "            #Computes the indices in the tensor corresponding to the sensor coordinates\n",
    "            tensor_ind=self.Tensor_ind_from_coord(sensor_xyz[0], sensor_xyz[1], sensor_xyz[2]) \n",
    "            \n",
    "            #If this sensor has not been activated before, remember that this is the first time\n",
    "            if( formated_pulse[0,tensor_ind[0],tensor_ind[1],tensor_ind[2]]==0):\n",
    "                formated_pulse[0,tensor_ind[0],tensor_ind[1],tensor_ind[2]]= pulse[1] - meta_vals[2]\n",
    "            \n",
    "            else:\n",
    "            #Possible last time, will be the last time for the actual last one\n",
    "                formated_pulse[1,tensor_ind[0],tensor_ind[1],tensor_ind[2]]=pulse[1] - meta_vals[2] # first time\n",
    "            \n",
    "            #Add charge\n",
    "            formated_pulse[2,tensor_ind[0],tensor_ind[1],tensor_ind[2]] += pulse[2]\n",
    "            \n",
    "            #Compute the distance between the sensor and the furthest possible cascade from loudest bang.\n",
    "            scatter=np.linalg.norm( np.array([sensor_xyz[0], sensor_xyz[1], sensor_xyz[2]])-loudest[2])-0.23*(loudest[1]-pulse[1])\n",
    "            formated_pulse[3,tensor_ind[0],tensor_ind[1],tensor_ind[2]]= scatter\n",
    "            \n",
    "            #Include pre-computed azimuth, zenith and number of cluster\n",
    "            az_t_pred = pre_feature.loc[pre_feature['event_id']==event_id]['az_t_pred'].iloc[0]\n",
    "            ze_t_pred = pre_feature.loc[pre_feature['event_id']==event_id]['ze_t_pred'].iloc[0]\n",
    "            num_clusters= pre_feature.loc[pre_feature['event_id']==event_id]['num_clusters'].iloc[0]\n",
    "            \n",
    "            \n",
    "            pre_comp=np.array([az_t_pred,ze_t_pred, num_clusters])\n",
    "            pre_comp=torch.from_numpy(pre_comp).to(device)\n",
    "                \n",
    "            return ([formated_pulse,pre_comp], torch.from_numpy(meta_vals[-2:]).to(device)) #returns the formated pulse and the meta-values\n",
    "    \n",
    "    #Function which computes the pulse with maximum charge of a given event and outputs its sensor_id, time and position\n",
    "    def loudest_bang(self, event):\n",
    "        charges=event.charge.values\n",
    "        sensors=event.sensor_id.values\n",
    "        times=event.time.values\n",
    "        i=charges.argmax(axis=0)\n",
    "        sen_max=sensors[i]\n",
    "        time_max=times[i]\n",
    "        xyz_from_id=self.id_to_xyz(sen_max)\n",
    "        max_pos=[xyz_from_id[0], xyz_from_id[1], xyz_from_id[2]]\n",
    "            \n",
    "        return [sen_max,time_max,max_pos]\n",
    "        \n",
    "        \n",
    "    #Computes xyz-coordinates based of sensor based on sensor_id   \n",
    "    def id_to_xyz(self, sen):\n",
    "        row = tuple(self.sensor_geom.loc[sen][1:4])\n",
    "        return row\n",
    "        \n",
    "        \n",
    "    #Computes indices of sensor in tensor based on xyz-coordinates   \n",
    "    def Tensor_ind_from_coord(self, x,y,z):\n",
    "        tensor_ind_from_coord=[int(x/50),int(y/50),int(z/5)]\n",
    "        return tensor_ind_from_coord\n",
    "    \n",
    "    #Saves all the xyz-coordinates of the sensors (which is event independent)\n",
    "    def get_sensor_pos(self):\n",
    "        x_values=np.zeros(0)\n",
    "        y_values=np.zeros(0)\n",
    "        z_values=np.zeros(0)\n",
    "        for i in range(1,5160):\n",
    "            pos=self.id_to_xyz(i)\n",
    "            pos=np.array([pos[0],pos[1],pos[2]])\n",
    "            x_values=np.append(x_values,pos[0])\n",
    "            y_values=np.append(y_values,pos[1])\n",
    "            z_values=np.append(z_values,pos[2])\n",
    "   \n",
    "\n",
    "        x_values=np.array(sorted(set(x_values)))\n",
    "        x_values= x_values+570.9 #22 values if downsampel3ed by 50m\n",
    "        y_values=np.array(sorted(set(y_values)))\n",
    "        y_values= y_values+521.08 #20 values if downsampeled by 50 m\n",
    "        z_values=np.array(sorted(set(z_values)))\n",
    "        z_values=z_values+512.82\n",
    "        return [x_values, y_values, z_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0c54a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our custom loss class\n",
    "class custom_MAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_MAE, self).__init__();\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        loss_value = self.angular_dist_score(predictions, target).to(device)\n",
    "        return loss_value\n",
    "    \n",
    "    #This is the scoring metric provided by Kaggle\n",
    "    def angular_dist_score(self, predictions, true):\n",
    "        '''\n",
    "        calculate the MAE of the angular distance between two directions.\n",
    "        The two vectors are first converted to cartesian unit vectors,\n",
    "        and then their scalar product is computed, which is equal to\n",
    "        the cosine of the angle between the two vectors. The inverse \n",
    "        cosine (arccos) thereof is then the angle between the two input vectors\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "    \n",
    "        az_true : float (or array thereof)\n",
    "            true azimuth value(s) in radian\n",
    "        zen_true : float (or array thereof)\n",
    "            true zenith value(s) in radian\n",
    "        az_pre : float (or array thereof)\n",
    "            predicted azimuth value(s) in radian\n",
    "        zen_pre : float (or array thereof)\n",
    "            predicted zenith value(s) in radian\n",
    "    \n",
    "        Returns:\n",
    "        --------\n",
    "    \n",
    "        dist : float\n",
    "            mean over the angular distance(s) in radian\n",
    "        '''\n",
    "    \n",
    "        az_true=true[:,0].to(device)\n",
    "        zen_true=true[:,1].to(device)\n",
    "        az_pred=predictions[:,0].to(device)\n",
    "        zen_pred=predictions[:,1].to(device)\n",
    "    \n",
    "        if not (torch.all(torch.isfinite(az_true)) and\n",
    "                torch.all(torch.isfinite(zen_true)) and\n",
    "                torch.all(torch.isfinite(az_pred)) and\n",
    "                torch.all(torch.isfinite(zen_pred))):\n",
    "            raise ValueError(\"All arguments must be finite\")\n",
    "    \n",
    "        # pre-compute all sine and cosine values\n",
    "        sa1 = torch.sin(az_true).to(device)\n",
    "        ca1 = torch.cos(az_true).to(device)\n",
    "        sz1 = torch.sin(zen_true).to(device)\n",
    "        cz1 = torch.cos(zen_true).to(device)\n",
    "    \n",
    "        sa2 = torch.sin(az_pred).to(device)\n",
    "        ca2 = torch.cos(az_pred).to(device)\n",
    "        sz2 = torch.sin(zen_pred).to(device)\n",
    "        cz2 = torch.cos(zen_pred).to(device)\n",
    "    \n",
    "        # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "        scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
    "    \n",
    "        # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
    "        # that might otherwise occure from the finite precision of the sine and cosine functions\n",
    "        scalar_prod =  torch.clip(scalar_prod, -1, 1)\n",
    "    \n",
    "        # convert back to an angle (in radian)\n",
    "        return torch.mean(torch.abs(torch.arccos(scalar_prod))).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840841b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2eb72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define the Convolutional Neural Network\n",
    "This expects a tensor with four features/channels\n",
    "We apply two cycles of convolutional layers followed by Max-Pool layers\n",
    "After this we flaten the tensor and add in the pre-computed best-fit azimuth, zenith and number of clusters \n",
    "Then we apply three linear layers with ReLu-activation to shrink it down to our azimuth and zenith prediction.\n",
    "'''\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,  use_activation = True ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1= nn.Conv3d(4,30,4)\n",
    "        self.pool1=  nn.MaxPool3d(4,4)\n",
    "        self.conv2= nn.Conv3d(30,20,2)\n",
    "        self.pool2= nn.MaxPool3d(2,2)\n",
    "        self.fc1=   nn.Linear(503,100, dtype=float)\n",
    "        self.fc2=   nn.Linear(100,50, dtype=float)\n",
    "        self.fc3=   nn.Linear(50,10, dtype= float)\n",
    "        self.fc4=   nn.Linear(10,2, dtype=float)\n",
    "        \n",
    "    \n",
    "    def forward(self,X):\n",
    "        x=X[0]\n",
    "        pre_comp=X[1]\n",
    "        x=F.tanh(self.conv1(x)) \n",
    "        x=self.pool1(x)\n",
    "        x=F.tanh(self.conv2(x))\n",
    "        x=self.pool2(x)\n",
    "        x=torch.flatten(x,1)\n",
    "        x=torch.cat((x,pre_comp),1)\n",
    "        x=F.tanh(self.fc1(x))\n",
    "        x=F.tanh(self.fc2(x))\n",
    "        x=F.tanh(self.fc3(x))\n",
    "        x=self.fc4(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeaa589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Training Loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch, lr, bs):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode\n",
    "    model=model.train()\n",
    "    loss_list=np.empty(0)\n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "        # Compute preiction and loss\n",
    "        pred = model(X).to(device)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Format training output\n",
    "        new_loss, current = loss.item(), (batch + 1) * len(X[0])\n",
    "        loss_list=np.append(loss_list, loss.item())\n",
    "        if (batch % 1000 == 0 ):\n",
    "            loss_list=loss_list[-1000:]\n",
    "            loss=np.mean(loss_list)\n",
    "            print(f\"epoch: {epoch:>2d}, lr: {lr:>2f}, batch_size: {bs:>5d}, loss: {loss:>f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3905feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loop(model, val_set):\n",
    "    #Load the validation data\n",
    "    validation_dataloader=DataLoader(val_set, batch_size=5, shuffle=False, num_workers=0, generator=torch.Generator(device=device))\n",
    "    #initialize the loss and number of events\n",
    "    loss_total = 0\n",
    "    num = 0\n",
    "    #Set the model to evaluate\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch, (X, y) in enumerate(validation_dataloader):\n",
    "            # Compute preiction and loss\n",
    "            pred = model(X)\n",
    "            loss_total += loss_fn.angular_dist_score(pred,y)\n",
    "            num +=1\n",
    "            mean=loss_total/num\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "870ae4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set epoch, batch size, learning rate, loss_fn. \n",
    "epoch=20\n",
    "batch_size=10\n",
    "learning_rate = 1e-4\n",
    "loss_fn = custom_MAE()\n",
    "\n",
    "#Set choice of training batch\n",
    "batch_id=10\n",
    "batch_path=batch_dir+\"batch_\"+str(batch_id)+\".parquet\"\n",
    "\n",
    "#Load dataset, setup data splits, and set manual seed\n",
    "dataset = NeutrinoDataset(batch_path, sensor_geom_path, batch_id, aux=True)\n",
    "#sub_dataset=torch.utils.data.Subset(dataset, np.arange(100000))\n",
    "train=torch.utils.data.Subset(dataset, np.arange(150000))\n",
    "test=torch.utils.data.Subset(dataset, np.arange(150000, 200000))\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Initalize the NN, optimizer, dataloader. \n",
    "model = ConvNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=0, generator=torch.Generator(device=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f7d877",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.057302  [   10/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.528467  [10010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.429699  [20010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.339994  [30010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.312941  [40010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.275247  [50010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.269443  [60010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.274805  [70010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.247268  [80010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.263310  [90010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.263639  [100010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.236429  [110010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.255628  [120010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.254956  [130010/150000]\n",
      "epoch:  0, lr: 0.000100, batch_size:    10, loss: 1.248832  [140010/150000]\n",
      "Validation MAE: 1.238754.\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.172953  [   10/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.237511  [10010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.233589  [20010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.225121  [30010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.233063  [40010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.224956  [50010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.226857  [60010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.233410  [70010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.210229  [80010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.230525  [90010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.232747  [100010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.211146  [110010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.230238  [120010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.230451  [130010/150000]\n",
      "epoch:  1, lr: 0.000100, batch_size:    10, loss: 1.226683  [140010/150000]\n",
      "Validation MAE: 1.221760.\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.122547  [   10/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.221417  [10010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.218804  [20010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.211085  [30010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.220373  [40010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.215727  [50010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.218613  [60010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.225643  [70010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.204862  [80010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.225550  [90010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.227943  [100010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.207697  [110010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.227464  [120010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.227119  [130010/150000]\n",
      "epoch:  2, lr: 0.000100, batch_size:    10, loss: 1.224619  [140010/150000]\n",
      "Validation MAE: 1.221578.\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.112381  [   10/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.220207  [10010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.217561  [20010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.208972  [30010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.219024  [40010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.214248  [50010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.216916  [60010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.224870  [70010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.203632  [80010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.224478  [90010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.226728  [100010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.206928  [110010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.226327  [120010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.226223  [130010/150000]\n",
      "epoch:  3, lr: 0.000100, batch_size:    10, loss: 1.223728  [140010/150000]\n",
      "Validation MAE: 1.221049.\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.108267  [   10/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.219398  [10010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.216899  [20010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.208042  [30010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.218587  [40010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.213731  [50010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.216208  [60010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.224264  [70010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.203000  [80010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.223944  [90010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.225799  [100010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.206342  [110010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.225985  [120010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.225691  [130010/150000]\n",
      "epoch:  4, lr: 0.000100, batch_size:    10, loss: 1.223320  [140010/150000]\n",
      "Validation MAE: 1.220573.\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.103310  [   10/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.219109  [10010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.216316  [20010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.207636  [30010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.218016  [40010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.213115  [50010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.215604  [60010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.223894  [70010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.202478  [80010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.223195  [90010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.225531  [100010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.205866  [110010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.225606  [120010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.225231  [130010/150000]\n",
      "epoch:  5, lr: 0.000100, batch_size:    10, loss: 1.222816  [140010/150000]\n",
      "Validation MAE: 1.220646.\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.097958  [   10/150000]\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.218657  [10010/150000]\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.215864  [20010/150000]\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.207099  [30010/150000]\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.217842  [40010/150000]\n",
      "epoch:  6, lr: 0.000100, batch_size:    10, loss: 1.212802  [50010/150000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epoch):\n",
    "    model=train_loop(train_dataloader, model, loss_fn, optimizer, i, learning_rate, batch_size)\n",
    "    mean=validate_loop(model, test)\n",
    "    print(f\"Validation MAE: {mean:>5f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4a667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
